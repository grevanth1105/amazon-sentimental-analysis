{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":800230,"sourceType":"datasetVersion","datasetId":1305},{"sourceId":10479620,"sourceType":"datasetVersion","datasetId":6489054}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import fireducks.pandas as pd\nimport os\n\n# Path to the extracted chunk files (from your Kaggle dataset structure)\nextracted_chunks_path = \"/kaggle/input/processed-chunks-1\"  # Adjust if the path differs\n\n# Combine all chunk files\nall_chunks = []\nfor file_name in sorted(os.listdir(extracted_chunks_path)):  # Ensure files are combined in order\n    if file_name.startswith(\"processed_chunk_\") and file_name.endswith(\".csv\"):\n        file_path = os.path.join(extracted_chunks_path, file_name)\n        print(f\"Loading {file_name}...\")\n        chunk = pd.read_csv(file_path)\n        all_chunks.append(chunk)\n\n# Concatenate all chunks into a single DataFrame\ncombined_data = pd.concat(all_chunks, ignore_index=True)\n\n# Display combined data info\nprint(\"Combined data shape:\", combined_data.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:14:03.680867Z","iopub.execute_input":"2025-03-16T15:14:03.681240Z","iopub.status.idle":"2025-03-16T15:14:25.525426Z","shell.execute_reply.started":"2025-03-16T15:14:03.681207Z","shell.execute_reply":"2025-03-16T15:14:25.524403Z"}},"outputs":[{"name":"stdout","text":"Loading processed_chunk_0_50000.csv...\nLoading processed_chunk_1000000_1050000.csv...\nLoading processed_chunk_100000_150000.csv...\nLoading processed_chunk_1050000_1100000.csv...\nLoading processed_chunk_1100000_1150000.csv...\nLoading processed_chunk_1150000_1200000.csv...\nLoading processed_chunk_1200000_1250000.csv...\nLoading processed_chunk_1250000_1300000.csv...\nLoading processed_chunk_1300000_1350000.csv...\nLoading processed_chunk_1350000_1400000.csv...\nLoading processed_chunk_1400000_1450000.csv...\nLoading processed_chunk_1450000_1500000.csv...\nLoading processed_chunk_1500000_1550000.csv...\nLoading processed_chunk_150000_200000.csv...\nLoading processed_chunk_1550000_1600000.csv...\nLoading processed_chunk_1600000_1650000.csv...\nLoading processed_chunk_1650000_1700000.csv...\nLoading processed_chunk_1700000_1750000.csv...\nLoading processed_chunk_1750000_1800000.csv...\nLoading processed_chunk_1800000_1850000.csv...\nLoading processed_chunk_1850000_1900000.csv...\nLoading processed_chunk_1900000_1950000.csv...\nLoading processed_chunk_1950000_2000000.csv...\nLoading processed_chunk_2000000_2050000.csv...\nLoading processed_chunk_200000_250000.csv...\nLoading processed_chunk_2050000_2100000.csv...\nLoading processed_chunk_2100000_2150000.csv...\nLoading processed_chunk_2150000_2200000.csv...\nLoading processed_chunk_2200000_2250000.csv...\nLoading processed_chunk_2250000_2300000.csv...\nLoading processed_chunk_2300000_2350000.csv...\nLoading processed_chunk_2350000_2400000.csv...\nLoading processed_chunk_2400000_2450000.csv...\nLoading processed_chunk_2450000_2500000.csv...\nLoading processed_chunk_2500000_2550000.csv...\nLoading processed_chunk_250000_300000.csv...\nLoading processed_chunk_2550000_2600000.csv...\nLoading processed_chunk_2600000_2650000.csv...\nLoading processed_chunk_2650000_2700000.csv...\nLoading processed_chunk_2700000_2750000.csv...\nLoading processed_chunk_2750000_2800000.csv...\nLoading processed_chunk_2800000_2850000.csv...\nLoading processed_chunk_2850000_2900000.csv...\nLoading processed_chunk_2900000_2950000.csv...\nLoading processed_chunk_2950000_3000000.csv...\nLoading processed_chunk_3000000_3050000.csv...\nLoading processed_chunk_300000_350000.csv...\nLoading processed_chunk_3050000_3100000.csv...\nLoading processed_chunk_3100000_3150000.csv...\nLoading processed_chunk_3150000_3200000.csv...\nLoading processed_chunk_3200000_3250000.csv...\nLoading processed_chunk_3250000_3300000.csv...\nLoading processed_chunk_3300000_3350000.csv...\nLoading processed_chunk_3350000_3400000.csv...\nLoading processed_chunk_3400000_3450000.csv...\nLoading processed_chunk_3450000_3500000.csv...\nLoading processed_chunk_3500000_3550000.csv...\nLoading processed_chunk_350000_400000.csv...\nLoading processed_chunk_3550000_3600000.csv...\nLoading processed_chunk_400000_450000.csv...\nLoading processed_chunk_450000_500000.csv...\nLoading processed_chunk_500000_550000.csv...\nLoading processed_chunk_50000_100000.csv...\nLoading processed_chunk_550000_600000.csv...\nLoading processed_chunk_600000_650000.csv...\nLoading processed_chunk_650000_700000.csv...\nLoading processed_chunk_700000_750000.csv...\nLoading processed_chunk_750000_800000.csv...\nLoading processed_chunk_800000_850000.csv...\nLoading processed_chunk_850000_900000.csv...\nLoading processed_chunk_900000_950000.csv...\nLoading processed_chunk_950000_1000000.csv...\nCombined data shape: (3600000, 3)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Save the combined dataset as a CSV for future use\ncombined_data_path = \"/kaggle/working/combined_processed_data.csv\"\ncombined_data.to_csv(combined_data_path, index=False)\nprint(f\"Combined data saved at: {combined_data_path}\")\n# Load the saved combined dataset\ncombined_data = pd.read_csv(\"/kaggle/working/combined_processed_data.csv\")\n\n# Check the dataset structure\nprint(combined_data.info())\nprint(combined_data.head())\n# Check label distribution\nprint(combined_data['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:14:25.526902Z","iopub.execute_input":"2025-03-16T15:14:25.527298Z","iopub.status.idle":"2025-03-16T15:14:44.004627Z","shell.execute_reply.started":"2025-03-16T15:14:25.527274Z","shell.execute_reply":"2025-03-16T15:14:44.003517Z"}},"outputs":[{"name":"stdout","text":"Combined data saved at: /kaggle/working/combined_processed_data.csv\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3600000 entries, 0 to 3599999\nData columns (total 3 columns):\n #   Column          Dtype \n---  ------          ----- \n 0   review          object\n 1   label           int64 \n 2   cleaned_review  object\ndtypes: int64(1), object(2)\nmemory usage: 82.4+ MB\nNone\n                                              review  label  \\\n0  Stuning even for the non-gamer: This sound tra...      2   \n1  The best soundtrack ever to anything.: I'm rea...      2   \n2  Amazing!: This soundtrack is my favorite music...      2   \n3  Excellent Soundtrack: I truly like this soundt...      2   \n4  Remember, Pull Your Jaw Off The Floor After He...      2   \n\n                                      cleaned_review  \n0  stun non gamer sound track beautiful paint sen...  \n1  good soundtrack read lot review say good game ...  \n2  amazing soundtrack favorite music time hand in...  \n3  excellent soundtrack truly like soundtrack enj...  \n4  remember pull Jaw Floor hear play game know di...  \nlabel\n2    1800000\n1    1800000\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX = combined_data['cleaned_review']  # Features (cleaned reviews)\ny = combined_data['label']           # Labels (1 for neutral, 2 for positive)\n\n# Perform train-test split (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Testing samples: {X_test.shape[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:14:44.006267Z","iopub.execute_input":"2025-03-16T15:14:44.006594Z","iopub.status.idle":"2025-03-16T15:14:59.639523Z","shell.execute_reply.started":"2025-03-16T15:14:44.006561Z","shell.execute_reply":"2025-03-16T15:14:59.637468Z"}},"outputs":[{"name":"stdout","text":"Training samples: 2880000\nTesting samples: 720000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# FASTTEXT","metadata":{}},{"cell_type":"code","source":"import fasttext\nimport fasttext.util\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import ttest_rel\nimport psutil\nimport multiprocessing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:14:59.642136Z","iopub.execute_input":"2025-03-16T15:14:59.642850Z","iopub.status.idle":"2025-03-16T15:15:03.787279Z","shell.execute_reply.started":"2025-03-16T15:14:59.642802Z","shell.execute_reply":"2025-03-16T15:15:03.786206Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- FastText Vector Generation ---\n# Assuming X_train, X_test are defined (e.g., from your dataset)\n# Remove missing values\nX_train = X_train.fillna(\"\")\nX_test = X_test.fillna(\"\")\n\n# Preprocess data with minimal overhead\ndef preprocess_text(text):\n    return text.lower()\n\nprint(\"Starting preprocessing...\")\nN_CORES = max(1, multiprocessing.cpu_count() // 2)  # Use half the cores for balance\nX_train_processed = [preprocess_text(text) for text in tqdm(X_train, desc=\"Processing X_train\")]\nX_test_processed = [preprocess_text(text) for text in tqdm(X_test, desc=\"Processing X_test\")]\nprint(\"Preprocessing complete!\")\n\n# Save data in FastText format\ntrain_file = \"train.txt\"\ntest_file = \"test.txt\"\n\nprint(\"Writing training data to file...\")\nwith open(train_file, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(f\"__label__dummy {text}\" for text in X_train_processed) + \"\\n\")\n\nprint(\"Writing test data to file...\")\nwith open(test_file, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(f\"__label__dummy {text}\" for text in X_test_processed) + \"\\n\")\n\n# Train FastText model\nprint(\"Training FastText model...\")\nmodel = fasttext.train_unsupervised(\n    train_file,\n    model=\"skipgram\",\n    dim=50,          # Reduced dimensionality\n    epoch=3,         # Fewer epochs\n    minCount=5,      # Ignore rare words\n    thread=N_CORES,  # Multi-threading within FastText\n    lr=0.1,          # Faster convergence\n    bucket=2000000   # Smaller bucket size\n)\nprint(\"FastText training complete!\")\n\n# Generate sentence embeddings without joblib parallelization\ndef get_fasttext_vector(text):\n    return model.get_sentence_vector(text)\n\nprint(\"Converting text to FastText embeddings...\")\n# Use list comprehension with tqdm for progress tracking\nX_train_fasttext = [get_fasttext_vector(text) for text in tqdm(X_train_processed, desc=\"Embedding X_train\")]\nX_test_fasttext = [get_fasttext_vector(text) for text in tqdm(X_test_processed, desc=\"Embedding X_test\")]\nprint(\"FastText transformation complete!\")\n\n# Convert to numpy arrays for evaluation\nX_train_vectors = np.array(X_train_fasttext)\nX_test_vectors = np.array(X_test_fasttext)\n\n# Check shape\nprint(f\"Train FastText shape: {X_train_vectors.shape}\")\nprint(f\"Test FastText shape: {X_test_vectors.shape}\")\n\n# --- Six-Model Evaluation ---\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Ensure labels are numeric and 0-based\ny_train = np.array([int(label) for label in y_train])\ny_test = np.array([int(label) for label in y_test])\ny_train_adjusted = y_train - 1 if np.min(y_train) > 0 else y_train\ny_test_adjusted = y_test - 1 if np.min(y_test) > 0 else y_test\n\n# Define models\nN_CORES = multiprocessing.cpu_count() - 1\nmodels = {\n    'SVM': LinearSVC(C=1.0, max_iter=5000, dual=False, random_state=42),\n    'Logistic Regression': LogisticRegression(C=1.0, max_iter=5000, n_jobs=N_CORES, random_state=42),\n    'Gaussian NB': GaussianNB(),\n    'MLP Classifier': MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, learning_rate_init=0.001, random_state=42),\n    'XGBoost': XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, n_jobs=N_CORES, use_label_encoder=False, eval_metric='logloss', tree_method='hist', random_state=42),\n    'LightGBM': lgb.LGBMClassifier(n_estimators=200, max_depth=5, n_jobs=N_CORES, random_state=42, verbose=-1)\n}\n\n# Step 1: Train models and find top 2 by accuracy\naccuracies = {}\nprint(\"Training and evaluating models on test set...\")\nfor name, model in models.items():\n    subset_size = min(50000, X_train_vectors.shape[0])\n    model.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n    y_pred = model.predict(X_test_vectors)\n    if np.min(y_train) > 0:\n        y_pred = y_pred + 1\n    acc = accuracy_score(y_test, y_pred)\n    accuracies[name] = acc\n    print(f\"{name} Accuracy: {acc:.4f}\")\ncheck_memory()\n\n# Identify top 2 models\ntop_models = sorted(accuracies.items(), key=lambda x: x[1], reverse=True)[:2]\nprint(\"\\nTop 2 models by accuracy:\")\nfor name, acc in top_models:\n    print(f\"{name}: {acc:.4f}\")\n\n# Step 2: 10-Fold Cross-Validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nmetrics = ['accuracy', 'f1', 'roc_auc']\nresults = {name: {metric: [] for metric in metrics} for name in models.keys()}\n\nprint(\"\\nPerforming 10-fold cross-validation...\")\nfor name, model in models.items():\n    print(f\"Cross-validating {name}...\")\n    for train_idx, val_idx in kf.split(X_train_vectors):\n        X_train_fold, X_val_fold = X_train_vectors[train_idx], X_train_vectors[val_idx]\n        y_train_fold, y_val_fold = y_train_adjusted[train_idx], y_train_adjusted[val_idx]\n        model.fit(X_train_fold[:subset_size], y_train_fold[:subset_size])\n        y_pred_fold = model.predict(X_val_fold)\n        if np.min(y_train) > 0:\n            y_pred_fold = y_pred_fold + 1\n            y_val_fold = y_val_fold + 1\n        results[name]['accuracy'].append(accuracy_score(y_val_fold, y_pred_fold))\n        results[name]['f1'].append(f1_score(y_val_fold, y_pred_fold, average='weighted'))\n        if hasattr(model, \"predict_proba\"):\n            y_prob_fold = model.predict_proba(X_val_fold)[:, 1]\n            results[name]['roc_auc'].append(roc_auc_score(y_val_fold, y_prob_fold))\n        else:\n            results[name]['roc_auc'].append(np.nan)\n\n# Format cross-validation results\nprint(\"\\n### 1. Perform K-Fold Cross-Validation and Report Detailed Performance Metrics\")\nprint(\"Table: Performance of Models with FastText Features (10-Fold Cross-Validation)\")\nprint(\"| Model              | Accuracy (Mean ± SD) | F1-Score (Mean ± SD) | AUC-ROC (Mean ± SD) |\")\nprint(\"|--------------------|---------------------|---------------------|--------------------|\")\nfor name in models.keys():\n    acc_mean, acc_std = np.mean(results[name]['accuracy']), np.std(results[name]['accuracy'])\n    f1_mean, f1_std = np.mean(results[name]['f1']), np.std(results[name]['f1'])\n    auc_mean, auc_std = np.mean(results[name]['roc_auc']), np.std(results[name]['roc_auc'])\n    print(f\"| {name:<18} | {acc_mean:.2f} ± {acc_std:.2f}         | {f1_mean:.2f} ± {f1_std:.2f}         | {auc_mean:.2f} ± {auc_std:.2f}        |\")\n\n# Step 3: Statistical Significance Tests\ntop_model_1_name, top_model_2_name = top_models[0][0], top_models[1][0]\ntop_model_1_scores = results[top_model_1_name]['accuracy']\ntop_model_2_scores = results[top_model_2_name]['accuracy']\n\nprint(\"\\n### 2. Conduct Statistical Significance Tests and Present p-Values\")\nprint(f\"Table: Statistical Significance of {top_model_1_name} and {top_model_2_name} vs. Other Models (FastText)\")\nprint(\"| Comparison                  | p-value (Accuracy) | Significant? |\")\nprint(\"|-----------------------------|-------------------|--------------|\")\nfor name in models.keys():\n    if name != top_model_1_name:\n        other_scores = results[name]['accuracy']\n        t_stat, p_value = ttest_rel(top_model_1_scores, other_scores)\n        significant = \"Yes\" if p_value < 0.05 else \"No\"\n        print(f\"| {top_model_1_name} vs. {name:<15} | {p_value:.3f}             | {significant:<12} |\")\nfor name in models.keys():\n    if name != top_model_2_name and name != top_model_1_name:\n        other_scores = results[name]['accuracy']\n        t_stat, p_value = ttest_rel(top_model_2_scores, other_scores)\n        significant = \"Yes\" if p_value < 0.05 else \"No\"\n        print(f\"| {top_model_2_name} vs. {name:<15} | {p_value:.3f}             | {significant:<12} |\")\ncheck_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:07:15.832405Z","iopub.execute_input":"2025-03-16T16:07:15.833102Z","iopub.status.idle":"2025-03-16T16:52:12.624939Z","shell.execute_reply.started":"2025-03-16T16:07:15.833046Z","shell.execute_reply":"2025-03-16T16:52:12.622951Z"}},"outputs":[{"name":"stdout","text":"Starting parallel preprocessing...\nPreprocessing complete!\nWriting training data to file...\nWriting test data to file...\nTraining FastText model...\nFastText training complete!\nConverting text to FastText embeddings...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)","\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/queues.py\", line 159, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/reduction.py\", line 215, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/reduction.py\", line 208, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 1245, in dump\n    return super().dump(obj)\nTypeError: cannot pickle 'fasttext_pybind.fasttext' object\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a82ffa299bb4>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Converting text to FastText embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mX_train_fasttext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_CORES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_fasttext_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mX_test_fasttext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_CORES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_fasttext_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FastText transformation complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."],"ename":"PicklingError","evalue":"Could not pickle the task to send it to the workers.","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}