{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":800230,"sourceType":"datasetVersion","datasetId":1305},{"sourceId":10479620,"sourceType":"datasetVersion","datasetId":6489054}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import fireducks.pandas as pd\nimport os\n\n# Path to the extracted chunk files (from your Kaggle dataset structure)\nextracted_chunks_path = \"/kaggle/input/processed-chunks-1\"  # Adjust if the path differs\n\n# Combine all chunk files\nall_chunks = []\nfor file_name in sorted(os.listdir(extracted_chunks_path)):  # Ensure files are combined in order\n    if file_name.startswith(\"processed_chunk_\") and file_name.endswith(\".csv\"):\n        file_path = os.path.join(extracted_chunks_path, file_name)\n        print(f\"Loading {file_name}...\")\n        chunk = pd.read_csv(file_path)\n        all_chunks.append(chunk)\n\n# Concatenate all chunks into a single DataFrame\ncombined_data = pd.concat(all_chunks, ignore_index=True)\n\n# Display combined data info\nprint(\"Combined data shape:\", combined_data.shape)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-16T08:16:47.300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the combined dataset as a CSV for future use\ncombined_data_path = \"/kaggle/working/combined_processed_data.csv\"\ncombined_data.to_csv(combined_data_path, index=False)\nprint(f\"Combined data saved at: {combined_data_path}\")\n# Load the saved combined dataset\ncombined_data = pd.read_csv(\"/kaggle/working/combined_processed_data.csv\")\n\n# Check the dataset structure\nprint(combined_data.info())\nprint(combined_data.head())\n# Check label distribution\nprint(combined_data['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:11:37.288237Z","iopub.execute_input":"2025-03-16T07:11:37.288741Z","iopub.status.idle":"2025-03-16T07:11:54.660952Z","shell.execute_reply.started":"2025-03-16T07:11:37.288710Z","shell.execute_reply":"2025-03-16T07:11:54.659899Z"}},"outputs":[{"name":"stdout","text":"Combined data saved at: /kaggle/working/combined_processed_data.csv\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3600000 entries, 0 to 3599999\nData columns (total 3 columns):\n #   Column          Dtype \n---  ------          ----- \n 0   review          object\n 1   label           int64 \n 2   cleaned_review  object\ndtypes: int64(1), object(2)\nmemory usage: 82.4+ MB\nNone\n                                              review  label  \\\n0  Stuning even for the non-gamer: This sound tra...      2   \n1  The best soundtrack ever to anything.: I'm rea...      2   \n2  Amazing!: This soundtrack is my favorite music...      2   \n3  Excellent Soundtrack: I truly like this soundt...      2   \n4  Remember, Pull Your Jaw Off The Floor After He...      2   \n\n                                      cleaned_review  \n0  stun non gamer sound track beautiful paint sen...  \n1  good soundtrack read lot review say good game ...  \n2  amazing soundtrack favorite music time hand in...  \n3  excellent soundtrack truly like soundtrack enj...  \n4  remember pull Jaw Floor hear play game know di...  \nlabel\n2    1800000\n1    1800000\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX = combined_data['cleaned_review']  # Features (cleaned reviews)\ny = combined_data['label']           # Labels (1 for neutral, 2 for positive)\n\n# Perform train-test split (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Testing samples: {X_test.shape[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:12:01.218149Z","iopub.execute_input":"2025-03-16T07:12:01.218481Z","iopub.status.idle":"2025-03-16T07:12:06.919818Z","shell.execute_reply.started":"2025-03-16T07:12:01.218455Z","shell.execute_reply":"2025-03-16T07:12:06.918751Z"}},"outputs":[{"name":"stdout","text":"Training samples: 2880000\nTesting samples: 720000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# FASTTEXT","metadata":{}},{"cell_type":"code","source":"import fasttext\nimport fasttext.util\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import ttest_rel\nimport psutil\nimport multiprocessing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:12:13.519003Z","iopub.execute_input":"2025-03-16T07:12:13.519494Z","iopub.status.idle":"2025-03-16T07:12:16.933888Z","shell.execute_reply.started":"2025-03-16T07:12:13.519465Z","shell.execute_reply":"2025-03-16T07:12:16.932771Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- FastText Vector Generation ---\n# Assuming X_train, X_test, y_train, y_test are defined (e.g., from your dataset)\n# Remove missing values\nX_train = X_train.fillna(\"\")\nX_test = X_test.fillna(\"\")\n\n# Preprocess data using parallel processing\ndef preprocess_text(text):\n    return text.lower()\n\nprint(\"Starting parallel preprocessing...\")\nX_train_processed = Parallel(n_jobs=-1)(delayed(preprocess_text)(text) for text in X_train)\nX_test_processed = Parallel(n_jobs=-1)(delayed(preprocess_text)(text) for text in X_test)\nprint(\"Preprocessing complete!\")\n\n# Save data in FastText format\ntrain_file = \"train.txt\"\ntest_file = \"test.txt\"\n\nwith open(train_file, \"w\", encoding=\"utf-8\") as f:\n    for text in X_train_processed:\n        f.write(f\"__label__dummy {text}\\n\")\n\nwith open(test_file, \"w\", encoding=\"utf-8\") as f:\n    for text in X_test_processed:\n        f.write(f\"__label__dummy {text}\\n\")\n\n# Train FastText model\nprint(\"Training FastText model...\")\nmodel = fasttext.train_unsupervised(train_file, model=\"skipgram\", dim=300)\nprint(\"FastText training complete!\")\n\n# Generate sentence embeddings\ndef get_fasttext_vector(text):\n    words = text.split()\n    vectors = [model.get_word_vector(word) for word in words if word in model.words]\n    return sum(vectors) / len(vectors) if vectors else [0] * 300\n\nprint(\"Converting text to FastText embeddings...\")\nX_train_fasttext = Parallel(n_jobs=-1)(delayed(get_fasttext_vector)(text) for text in X_train_processed)\nX_test_fasttext = Parallel(n_jobs=-1)(delayed(get_fasttext_vector)(text) for text in X_test_processed)\nprint(\"FastText transformation complete!\")\n\n# Convert to numpy arrays for evaluation\nX_train_vectors = np.array(X_train_fasttext)\nX_test_vectors = np.array(X_test_fasttext)\n\n# Check shape\nprint(f\"Train FastText shape: {X_train_vectors.shape}\")\nprint(f\"Test FastText shape: {X_test_vectors.shape}\")\n\n# --- Six-Model Evaluation ---\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Ensure labels are numeric and 0-based\ny_train = np.array([int(label) for label in y_train])\ny_test = np.array([int(label) for label in y_test])\ny_train_adjusted = y_train - 1 if np.min(y_train) > 0 else y_train\ny_test_adjusted = y_test - 1 if np.min(y_test) > 0 else y_test\n\n# Define models\nN_CORES = multiprocessing.cpu_count() - 1\nmodels = {\n    'SVM': LinearSVC(C=1.0, max_iter=5000, dual=False, random_state=42),\n    'Logistic Regression': LogisticRegression(C=1.0, max_iter=5000, n_jobs=N_CORES, random_state=42),\n    'Gaussian NB': GaussianNB(),\n    'MLP Classifier': MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, learning_rate_init=0.001, random_state=42),\n    'XGBoost': XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, n_jobs=N_CORES, use_label_encoder=False, eval_metric='logloss', tree_method='hist', random_state=42),\n    'LightGBM': lgb.LGBMClassifier(n_estimators=200, max_depth=5, n_jobs=N_CORES, random_state=42, verbose=-1)\n}\n\n# Step 1: Train models and find top 2 by accuracy\naccuracies = {}\nprint(\"Training and evaluating models on test set...\")\nfor name, model in models.items():\n    subset_size = min(50000, X_train_vectors.shape[0])\n    model.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n    y_pred = model.predict(X_test_vectors)\n    if np.min(y_train) > 0:\n        y_pred = y_pred + 1\n    acc = accuracy_score(y_test, y_pred)\n    accuracies[name] = acc\n    print(f\"{name} Accuracy: {acc:.4f}\")\ncheck_memory()\n\n# Identify top 2 models\ntop_models = sorted(accuracies.items(), key=lambda x: x[1], reverse=True)[:2]\nprint(\"\\nTop 2 models by accuracy:\")\nfor name, acc in top_models:\n    print(f\"{name}: {acc:.4f}\")\n\n# Step 2: 10-Fold Cross-Validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nmetrics = ['accuracy', 'f1', 'roc_auc']\nresults = {name: {metric: [] for metric in metrics} for name in models.keys()}\n\nprint(\"\\nPerforming 10-fold cross-validation...\")\nfor name, model in models.items():\n    print(f\"Cross-validating {name}...\")\n    for train_idx, val_idx in kf.split(X_train_vectors):\n        X_train_fold, X_val_fold = X_train_vectors[train_idx], X_train_vectors[val_idx]\n        y_train_fold, y_val_fold = y_train_adjusted[train_idx], y_train_adjusted[val_idx]\n        model.fit(X_train_fold[:subset_size], y_train_fold[:subset_size])\n        y_pred_fold = model.predict(X_val_fold)\n        if np.min(y_train) > 0:\n            y_pred_fold = y_pred_fold + 1\n            y_val_fold = y_val_fold + 1\n        results[name]['accuracy'].append(accuracy_score(y_val_fold, y_pred_fold))\n        results[name]['f1'].append(f1_score(y_val_fold, y_pred_fold, average='weighted'))\n        if hasattr(model, \"predict_proba\"):\n            y_prob_fold = model.predict_proba(X_val_fold)[:, 1]\n            results[name]['roc_auc'].append(roc_auc_score(y_val_fold, y_prob_fold))\n        else:\n            results[name]['roc_auc'].append(np.nan)\n\n# Format cross-validation results\nprint(\"\\n### 1. Perform K-Fold Cross-Validation and Report Detailed Performance Metrics\")\nprint(\"Table: Performance of Models with FastText Features (10-Fold Cross-Validation)\")\nprint(\"| Model              | Accuracy (Mean ± SD) | F1-Score (Mean ± SD) | AUC-ROC (Mean ± SD) |\")\nprint(\"|--------------------|---------------------|---------------------|--------------------|\")\nfor name in models.keys():\n    acc_mean, acc_std = np.mean(results[name]['accuracy']), np.std(results[name]['accuracy'])\n    f1_mean, f1_std = np.mean(results[name]['f1']), np.std(results[name]['f1'])\n    auc_mean, auc_std = np.mean(results[name]['roc_auc']), np.std(results[name]['roc_auc'])\n    print(f\"| {name:<18} | {acc_mean:.2f} ± {acc_std:.2f}         | {f1_mean:.2f} ± {f1_std:.2f}         | {auc_mean:.2f} ± {auc_std:.2f}        |\")\n\n# Step 3: Statistical Significance Tests\ntop_model_1_name, top_model_2_name = top_models[0][0], top_models[1][0]\ntop_model_1_scores = results[top_model_1_name]['accuracy']\ntop_model_2_scores = results[top_model_2_name]['accuracy']\n\nprint(\"\\n### 2. Conduct Statistical Significance Tests and Present p-Values\")\nprint(f\"Table: Statistical Significance of {top_model_1_name} and {top_model_2_name} vs. Other Models (FastText)\")\nprint(\"| Comparison                  | p-value (Accuracy) | Significant? |\")\nprint(\"|-----------------------------|-------------------|--------------|\")\nfor name in models.keys():\n    if name != top_model_1_name:\n        other_scores = results[name]['accuracy']\n        t_stat, p_value = ttest_rel(top_model_1_scores, other_scores)\n        significant = \"Yes\" if p_value < 0.05 else \"No\"\n        print(f\"| {top_model_1_name} vs. {name:<15} | {p_value:.3f}             | {significant:<12} |\")\nfor name in models.keys():\n    if name != top_model_2_name and name != top_model_1_name:\n        other_scores = results[name]['accuracy']\n        t_stat, p_value = ttest_rel(top_model_2_scores, other_scores)\n        significant = \"Yes\" if p_value < 0.05 else \"No\"\n        print(f\"| {top_model_2_name} vs. {name:<15} | {p_value:.3f}             | {significant:<12} |\")\ncheck_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:12:34.992014Z","iopub.execute_input":"2025-03-16T07:12:34.992819Z","execution_failed":"2025-03-16T08:16:47.299Z"}},"outputs":[{"name":"stdout","text":"Starting parallel preprocessing...\nPreprocessing complete!\nTraining FastText model...\n","output_type":"stream"}],"execution_count":null}]}