{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":800230,"sourceType":"datasetVersion","datasetId":1305},{"sourceId":10479620,"sourceType":"datasetVersion","datasetId":6489054}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import fireducks.pandas as pd\nimport os\n\n# Path to the extracted chunk files (from your Kaggle dataset structure)\nextracted_chunks_path = \"/kaggle/input/processed-chunks-1\"  # Adjust if the path differs\n\n# Combine all chunk files\nall_chunks = []\nfor file_name in sorted(os.listdir(extracted_chunks_path)):  # Ensure files are combined in order\n    if file_name.startswith(\"processed_chunk_\") and file_name.endswith(\".csv\"):\n        file_path = os.path.join(extracted_chunks_path, file_name)\n        print(f\"Loading {file_name}...\")\n        chunk = pd.read_csv(file_path)\n        all_chunks.append(chunk)\n\n# Concatenate all chunks into a single DataFrame\ncombined_data = pd.concat(all_chunks, ignore_index=True)\n\n# Display combined data info\nprint(\"Combined data shape:\", combined_data.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:35:38.607944Z","iopub.execute_input":"2025-03-11T16:35:38.608386Z","iopub.status.idle":"2025-03-11T16:35:55.466231Z","shell.execute_reply.started":"2025-03-11T16:35:38.608351Z","shell.execute_reply":"2025-03-11T16:35:55.465122Z"}},"outputs":[{"name":"stdout","text":"Loading processed_chunk_0_50000.csv...\nLoading processed_chunk_1000000_1050000.csv...\nLoading processed_chunk_100000_150000.csv...\nLoading processed_chunk_1050000_1100000.csv...\nLoading processed_chunk_1100000_1150000.csv...\nLoading processed_chunk_1150000_1200000.csv...\nLoading processed_chunk_1200000_1250000.csv...\nLoading processed_chunk_1250000_1300000.csv...\nLoading processed_chunk_1300000_1350000.csv...\nLoading processed_chunk_1350000_1400000.csv...\nLoading processed_chunk_1400000_1450000.csv...\nLoading processed_chunk_1450000_1500000.csv...\nLoading processed_chunk_1500000_1550000.csv...\nLoading processed_chunk_150000_200000.csv...\nLoading processed_chunk_1550000_1600000.csv...\nLoading processed_chunk_1600000_1650000.csv...\nLoading processed_chunk_1650000_1700000.csv...\nLoading processed_chunk_1700000_1750000.csv...\nLoading processed_chunk_1750000_1800000.csv...\nLoading processed_chunk_1800000_1850000.csv...\nLoading processed_chunk_1850000_1900000.csv...\nLoading processed_chunk_1900000_1950000.csv...\nLoading processed_chunk_1950000_2000000.csv...\nLoading processed_chunk_2000000_2050000.csv...\nLoading processed_chunk_200000_250000.csv...\nLoading processed_chunk_2050000_2100000.csv...\nLoading processed_chunk_2100000_2150000.csv...\nLoading processed_chunk_2150000_2200000.csv...\nLoading processed_chunk_2200000_2250000.csv...\nLoading processed_chunk_2250000_2300000.csv...\nLoading processed_chunk_2300000_2350000.csv...\nLoading processed_chunk_2350000_2400000.csv...\nLoading processed_chunk_2400000_2450000.csv...\nLoading processed_chunk_2450000_2500000.csv...\nLoading processed_chunk_2500000_2550000.csv...\nLoading processed_chunk_250000_300000.csv...\nLoading processed_chunk_2550000_2600000.csv...\nLoading processed_chunk_2600000_2650000.csv...\nLoading processed_chunk_2650000_2700000.csv...\nLoading processed_chunk_2700000_2750000.csv...\nLoading processed_chunk_2750000_2800000.csv...\nLoading processed_chunk_2800000_2850000.csv...\nLoading processed_chunk_2850000_2900000.csv...\nLoading processed_chunk_2900000_2950000.csv...\nLoading processed_chunk_2950000_3000000.csv...\nLoading processed_chunk_3000000_3050000.csv...\nLoading processed_chunk_300000_350000.csv...\nLoading processed_chunk_3050000_3100000.csv...\nLoading processed_chunk_3100000_3150000.csv...\nLoading processed_chunk_3150000_3200000.csv...\nLoading processed_chunk_3200000_3250000.csv...\nLoading processed_chunk_3250000_3300000.csv...\nLoading processed_chunk_3300000_3350000.csv...\nLoading processed_chunk_3350000_3400000.csv...\nLoading processed_chunk_3400000_3450000.csv...\nLoading processed_chunk_3450000_3500000.csv...\nLoading processed_chunk_3500000_3550000.csv...\nLoading processed_chunk_350000_400000.csv...\nLoading processed_chunk_3550000_3600000.csv...\nLoading processed_chunk_400000_450000.csv...\nLoading processed_chunk_450000_500000.csv...\nLoading processed_chunk_500000_550000.csv...\nLoading processed_chunk_50000_100000.csv...\nLoading processed_chunk_550000_600000.csv...\nLoading processed_chunk_600000_650000.csv...\nLoading processed_chunk_650000_700000.csv...\nLoading processed_chunk_700000_750000.csv...\nLoading processed_chunk_750000_800000.csv...\nLoading processed_chunk_800000_850000.csv...\nLoading processed_chunk_850000_900000.csv...\nLoading processed_chunk_900000_950000.csv...\nLoading processed_chunk_950000_1000000.csv...\nCombined data shape: (3600000, 3)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Save the combined dataset as a CSV for future use\ncombined_data_path = \"/kaggle/working/combined_processed_data.csv\"\ncombined_data.to_csv(combined_data_path, index=False)\nprint(f\"Combined data saved at: {combined_data_path}\")\n# Load the saved combined dataset\ncombined_data = pd.read_csv(\"/kaggle/working/combined_processed_data.csv\")\n\n# Check the dataset structure\nprint(combined_data.info())\nprint(combined_data.head())\n# Check label distribution\nprint(combined_data['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:35:55.468196Z","iopub.execute_input":"2025-03-11T16:35:55.468631Z","iopub.status.idle":"2025-03-11T16:36:14.089712Z","shell.execute_reply.started":"2025-03-11T16:35:55.468602Z","shell.execute_reply":"2025-03-11T16:36:14.088458Z"}},"outputs":[{"name":"stdout","text":"Combined data saved at: /kaggle/working/combined_processed_data.csv\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3600000 entries, 0 to 3599999\nData columns (total 3 columns):\n #   Column          Dtype \n---  ------          ----- \n 0   review          object\n 1   label           int64 \n 2   cleaned_review  object\ndtypes: int64(1), object(2)\nmemory usage: 82.4+ MB\nNone\n                                              review  label  \\\n0  Stuning even for the non-gamer: This sound tra...      2   \n1  The best soundtrack ever to anything.: I'm rea...      2   \n2  Amazing!: This soundtrack is my favorite music...      2   \n3  Excellent Soundtrack: I truly like this soundt...      2   \n4  Remember, Pull Your Jaw Off The Floor After He...      2   \n\n                                      cleaned_review  \n0  stun non gamer sound track beautiful paint sen...  \n1  good soundtrack read lot review say good game ...  \n2  amazing soundtrack favorite music time hand in...  \n3  excellent soundtrack truly like soundtrack enj...  \n4  remember pull Jaw Floor hear play game know di...  \nlabel\n2    1800000\n1    1800000\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX = combined_data['cleaned_review']  # Features (cleaned reviews)\ny = combined_data['label']           # Labels (1 for neutral, 2 for positive)\n\n# Perform train-test split (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Testing samples: {X_test.shape[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:36:14.091448Z","iopub.execute_input":"2025-03-11T16:36:14.091769Z","iopub.status.idle":"2025-03-11T16:36:20.057701Z","shell.execute_reply.started":"2025-03-11T16:36:14.091733Z","shell.execute_reply":"2025-03-11T16:36:20.056713Z"}},"outputs":[{"name":"stdout","text":"Training samples: 2880000\nTesting samples: 720000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# FASTTEXT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom gensim.models import FastText\nimport multiprocessing\nfrom tqdm import tqdm\nimport logging\n\n# Disable gensim logging to reduce I/O overhead\nlogging.getLogger('gensim').setLevel(logging.ERROR)\n\n# Parameters\nN_CORES = multiprocessing.cpu_count() - 1\nVECTOR_SIZE = 50  # Even smaller vectors for speed\nWINDOW = 3  # Smaller window\nMIN_COUNT = 10  # Higher threshold to reduce vocab size\nEPOCHS = 3  # Fewer epochs\nSAMPLE_SIZE = 100000  # Subsample if dataset is huge (adjust as needed)\n\n# Assuming X_train and X_test are lists\nprint(\"Preparing data...\")\n# Replace None or NaN with empty string in lists\nX_train = [\"\" if x is None or pd.isna(x) else x for x in X_train]\nX_test = [\"\" if x is None or pd.isna(x) else x for x in X_test]\n\n# Subsample if dataset is too large\nif len(X_train) > SAMPLE_SIZE:\n    print(f\"Subsampling training data to {SAMPLE_SIZE} samples...\")\n    X_train = np.random.choice(X_train, SAMPLE_SIZE, replace=False).tolist()\n\n# Simple preprocessing and tokenization in one step\nprint(\"Tokenizing data...\")\nX_train_tokenized = [str(text).lower().split() for text in tqdm(X_train)]\nX_test_tokenized = [str(text).lower().split() for text in tqdm(X_test)]\nprint(\"Tokenization complete!\")\n\n# Train a minimal FastText model\nprint(\"Training FastText model...\")\nfasttext_model = FastText(\n    vector_size=VECTOR_SIZE,\n    window=WINDOW,\n    min_count=MIN_COUNT,\n    workers=N_CORES,\n    sg=1,  # Skip-gram\n    hs=0,  # Negative sampling\n    negative=5,\n    seed=42,\n    sample=1e-4,  # Downsample frequent words\n)\n\n# Build vocab and train in one go\nfasttext_model.build_vocab(corpus_iterable=X_train_tokenized)\nfasttext_model.train(\n    corpus_iterable=X_train_tokenized,\n    total_examples=len(X_train_tokenized),\n    epochs=EPOCHS,\n)\nprint(\"FastText training complete!\")\n\n# Optimized vector averaging using numpy directly\ndef get_document_vector(tokens, model):\n    try:\n        return np.mean(model.wv[tokens], axis=0)\n    except:\n        return np.zeros(VECTOR_SIZE)\n\n# Vectorize using list comprehension instead of joblib\nprint(\"Transforming data to vectors...\")\nX_train_vectors = np.array([get_document_vector(tokens, fasttext_model) \n                          for tokens in tqdm(X_train_tokenized)])\nX_test_vectors = np.array([get_document_vector(tokens, fasttext_model) \n                         for tokens in tqdm(X_test_tokenized)])\nprint(\"Vector transformation complete!\")\n\n# Check shape\nprint(f\"Train vectors shape: {X_train_vectors.shape}\")\nprint(f\"Test vectors shape: {X_test_vectors.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:39:22.995843Z","iopub.execute_input":"2025-03-11T16:39:22.996413Z","iopub.status.idle":"2025-03-11T16:42:58.469644Z","shell.execute_reply.started":"2025-03-11T16:39:22.996368Z","shell.execute_reply":"2025-03-11T16:42:58.467077Z"}},"outputs":[{"name":"stdout","text":"Preparing data...\nTokenizing data...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100000/100000 [00:01<00:00, 78179.33it/s]\n100%|██████████| 720000/720000 [00:13<00:00, 51596.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Tokenization complete!\nTraining FastText model...\nFastText training complete!\nTransforming data to vectors...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100000/100000 [00:18<00:00, 5269.74it/s]\n100%|██████████| 720000/720000 [02:21<00:00, 5080.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Vector transformation complete!\nTrain vectors shape: (100000, 50)\nTest vectors shape: (720000, 50)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score\nimport psutil\nimport numpy as np\nimport multiprocessing\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Ensure labels are numeric (assuming y_train, y_test are lists from FastText pipeline)\ny_train = np.array([int(label) for label in y_train])\ny_test = np.array([int(label) for label in y_test])\n\n# If labels need to be 0-based (e.g., originally 1-based), adjust them\n# Comment out if your labels are already 0-based\ny_train_adjusted = y_train - 1 if np.min(y_train) > 0 else y_train\ny_test_adjusted = y_test - 1 if np.min(y_test) > 0 else y_test\n\n# Define LightGBM hyperparameters with reduced iterations and depth\nparam_grid = {\n    'learning_rate': [0.01, 0.05],\n    'n_estimators': [50, 100],\n    'max_depth': [3, 5],\n    'num_leaves': [15, 31]\n}\n\n# Stratified K-Fold Cross-Validation (3 folds for efficiency)\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\n# Use N_CORES from FastText code for consistency\nN_CORES = multiprocessing.cpu_count() - 1\n\n# Iterate over hyperparameters\nfor lr in param_grid['learning_rate']:\n    for iterations in param_grid['n_estimators']:\n        for depth in param_grid['max_depth']:\n            for num_leaves in param_grid['num_leaves']:\n                print(f\"Training LightGBM with learning_rate={lr}, iterations={iterations}, depth={depth}, num_leaves={num_leaves}...\")\n                \n                # Initialize the LightGBM model\n                lgbm_classifier = lgb.LGBMClassifier(\n                    learning_rate=lr,\n                    n_estimators=iterations,\n                    max_depth=depth,\n                    num_leaves=num_leaves,\n                    verbose=-1,  # Suppress output\n                    n_jobs=N_CORES  # Use same core count as FastText\n                )\n                \n                # Train on full FastText vectors (or subset if too large)\n                subset_size = min(10000, X_train_vectors.shape[0])  # Align with FastText output\n                lgbm_classifier.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n                \n                # Make predictions\n                y_pred_lgbm = lgbm_classifier.predict(X_test_vectors)\n                \n                # Adjust predictions back to original scale if labels were shifted\n                if np.min(y_train) > 0:\n                    y_pred_lgbm = y_pred_lgbm + 1\n                \n                # Performance evaluation\n                print(f\"LightGBM Classification Report (learning_rate={lr}, iterations={iterations}, depth={depth}, num_leaves={num_leaves}):\")\n                print(classification_report(y_test, y_pred_lgbm))\n                print(f\"LightGBM Accuracy: {accuracy_score(y_test, y_pred_lgbm):.4f}\\n\")\n                \n                # Check memory usage\n                check_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:43:15.068194Z","iopub.execute_input":"2025-03-11T16:43:15.068560Z","iopub.status.idle":"2025-03-11T16:44:04.924067Z","shell.execute_reply.started":"2025-03-11T16:43:15.068529Z","shell.execute_reply":"2025-03-11T16:44:04.922971Z"}},"outputs":[{"name":"stdout","text":"Training LightGBM with learning_rate=0.01, iterations=50, depth=3, num_leaves=15...\nLightGBM Classification Report (learning_rate=0.01, iterations=50, depth=3, num_leaves=15):\n              precision    recall  f1-score   support\n\n           1       0.50      0.66      0.57    359811\n           2       0.49      0.33      0.39    360189\n\n    accuracy                           0.50    720000\n   macro avg       0.50      0.50      0.48    720000\nweighted avg       0.50      0.50      0.48    720000\n\nLightGBM Accuracy: 0.4959\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.01, iterations=50, depth=3, num_leaves=31...\nLightGBM Classification Report (learning_rate=0.01, iterations=50, depth=3, num_leaves=31):\n              precision    recall  f1-score   support\n\n           1       0.50      0.66      0.57    359811\n           2       0.49      0.33      0.39    360189\n\n    accuracy                           0.50    720000\n   macro avg       0.50      0.50      0.48    720000\nweighted avg       0.50      0.50      0.48    720000\n\nLightGBM Accuracy: 0.4959\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.01, iterations=50, depth=5, num_leaves=15...\nLightGBM Classification Report (learning_rate=0.01, iterations=50, depth=5, num_leaves=15):\n              precision    recall  f1-score   support\n\n           1       0.49      0.65      0.56    359811\n           2       0.49      0.34      0.40    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.48    720000\nweighted avg       0.49      0.49      0.48    720000\n\nLightGBM Accuracy: 0.4924\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.01, iterations=50, depth=5, num_leaves=31...\nLightGBM Classification Report (learning_rate=0.01, iterations=50, depth=5, num_leaves=31):\n              precision    recall  f1-score   support\n\n           1       0.49      0.59      0.54    359811\n           2       0.49      0.40      0.44    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.49    720000\nweighted avg       0.49      0.49      0.49    720000\n\nLightGBM Accuracy: 0.4939\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.01, iterations=100, depth=3, num_leaves=15...\nLightGBM Classification Report (learning_rate=0.01, iterations=100, depth=3, num_leaves=15):\n              precision    recall  f1-score   support\n\n           1       0.49      0.60      0.54    359811\n           2       0.49      0.38      0.42    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.48    720000\nweighted avg       0.49      0.49      0.48    720000\n\nLightGBM Accuracy: 0.4890\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.01, iterations=100, depth=3, num_leaves=31...\nLightGBM Classification Report (learning_rate=0.01, iterations=100, depth=3, num_leaves=31):\n              precision    recall  f1-score   support\n\n           1       0.49      0.60      0.54    359811\n           2       0.49      0.38      0.42    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.48    720000\nweighted avg       0.49      0.49      0.48    720000\n\nLightGBM Accuracy: 0.4890\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.01, iterations=100, depth=5, num_leaves=15...\nLightGBM Classification Report (learning_rate=0.01, iterations=100, depth=5, num_leaves=15):\n              precision    recall  f1-score   support\n\n           1       0.49      0.60      0.54    359811\n           2       0.48      0.37      0.42    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.48    720000\nweighted avg       0.49      0.49      0.48    720000\n\nLightGBM Accuracy: 0.4872\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.01, iterations=100, depth=5, num_leaves=31...\nLightGBM Classification Report (learning_rate=0.01, iterations=100, depth=5, num_leaves=31):\n              precision    recall  f1-score   support\n\n           1       0.49      0.57      0.53    359811\n           2       0.49      0.40      0.44    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.48    720000\nweighted avg       0.49      0.49      0.48    720000\n\nLightGBM Accuracy: 0.4876\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.05, iterations=50, depth=3, num_leaves=15...\nLightGBM Classification Report (learning_rate=0.05, iterations=50, depth=3, num_leaves=15):\n              precision    recall  f1-score   support\n\n           1       0.48      0.52      0.50    359811\n           2       0.47      0.43      0.45    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nLightGBM Accuracy: 0.4746\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.05, iterations=50, depth=3, num_leaves=31...\nLightGBM Classification Report (learning_rate=0.05, iterations=50, depth=3, num_leaves=31):\n              precision    recall  f1-score   support\n\n           1       0.48      0.52      0.50    359811\n           2       0.47      0.43      0.45    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nLightGBM Accuracy: 0.4746\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.05, iterations=50, depth=5, num_leaves=15...\nLightGBM Classification Report (learning_rate=0.05, iterations=50, depth=5, num_leaves=15):\n              precision    recall  f1-score   support\n\n           1       0.48      0.52      0.50    359811\n           2       0.47      0.43      0.45    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nLightGBM Accuracy: 0.4743\n\nMemory usage: 45.2%\nTraining LightGBM with learning_rate=0.05, iterations=50, depth=5, num_leaves=31...\nLightGBM Classification Report (learning_rate=0.05, iterations=50, depth=5, num_leaves=31):\n              precision    recall  f1-score   support\n\n           1       0.48      0.53      0.50    359811\n           2       0.48      0.43      0.45    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nLightGBM Accuracy: 0.4803\n\nMemory usage: 45.3%\nTraining LightGBM with learning_rate=0.05, iterations=100, depth=3, num_leaves=15...\nLightGBM Classification Report (learning_rate=0.05, iterations=100, depth=3, num_leaves=15):\n              precision    recall  f1-score   support\n\n           1       0.47      0.50      0.49    359811\n           2       0.47      0.44      0.45    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nLightGBM Accuracy: 0.4699\n\nMemory usage: 45.3%\nTraining LightGBM with learning_rate=0.05, iterations=100, depth=3, num_leaves=31...\nLightGBM Classification Report (learning_rate=0.05, iterations=100, depth=3, num_leaves=31):\n              precision    recall  f1-score   support\n\n           1       0.47      0.50      0.49    359811\n           2       0.47      0.44      0.45    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nLightGBM Accuracy: 0.4699\n\nMemory usage: 45.3%\nTraining LightGBM with learning_rate=0.05, iterations=100, depth=5, num_leaves=15...\nLightGBM Classification Report (learning_rate=0.05, iterations=100, depth=5, num_leaves=15):\n              precision    recall  f1-score   support\n\n           1       0.47      0.49      0.48    359811\n           2       0.47      0.45      0.46    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nLightGBM Accuracy: 0.4690\n\nMemory usage: 45.3%\nTraining LightGBM with learning_rate=0.05, iterations=100, depth=5, num_leaves=31...\nLightGBM Classification Report (learning_rate=0.05, iterations=100, depth=5, num_leaves=31):\n              precision    recall  f1-score   support\n\n           1       0.48      0.49      0.48    359811\n           2       0.48      0.47      0.47    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nLightGBM Accuracy: 0.4766\n\nMemory usage: 45.3%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Catboost","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport psutil\nimport multiprocessing\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Ensure labels are numeric\ny_train = np.array([int(label) for label in y_train])\ny_test = np.array([int(label) for label in y_test])\n\n# Conditional label adjustment (assuming original labels are 1-based [1, 2])\ny_train_adjusted = y_train - 1 if np.min(y_train) > 0 else y_train\ny_test_adjusted = y_test - 1 if np.min(y_test) > 0 else y_test\n\n# Define hyperparameter grid\nparam_grid = {\n    'learning_rate': [0.05, 0.1],  # Slightly higher learning rates\n    'iterations': [100, 200],  # More iterations\n    'depth': [5, 7],  # Deeper trees\n    'l2_leaf_reg': [1, 3],\n    'border_count': [50, 100]  # More quantization levels\n}\n\n# Stratified K-Fold (increase to 5 folds for better validation)\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nN_CORES = multiprocessing.cpu_count() - 1\n\n# Iterate over hyperparameters\nfor lr in param_grid['learning_rate']:\n    for iterations in param_grid['iterations']:\n        for depth in param_grid['depth']:\n            for l2_leaf_reg in param_grid['l2_leaf_reg']:\n                for border_count in param_grid['border_count']:\n                    print(f\"Training CatBoost with learning_rate={lr}, iterations={iterations}, depth={depth}, l2_leaf_reg={l2_leaf_reg}, border_count={border_count}...\")\n                    \n                    catboost_classifier = CatBoostClassifier(\n                        learning_rate=lr,\n                        iterations=iterations,\n                        depth=depth,\n                        l2_leaf_reg=l2_leaf_reg,\n                        border_count=border_count,\n                        verbose=0,\n                        random_state=42,\n                        thread_count=N_CORES\n                    )\n                    \n                    # Train on more data (e.g., 50,000 samples or full dataset if memory allows)\n                    subset_size = min(50000, X_train_vectors.shape[0])\n                    catboost_classifier.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n                    \n                    # Make predictions\n                    y_pred_catboost = catboost_classifier.predict(X_test_vectors)\n                    if np.min(y_train) > 0:\n                        y_pred_catboost = y_pred_catboost + 1\n                    \n                    # Performance evaluation\n                    print(f\"CatBoost Classification Report (learning_rate={lr}, iterations={iterations}, depth={depth}, l2_leaf_reg={l2_leaf_reg}, border_count={border_count}):\")\n                    print(classification_report(y_test, y_pred_catboost))\n                    print(f\"CatBoost Accuracy: {accuracy_score(y_test, y_pred_catboost):.4f}\\n\")\n                    \n                    check_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:55:03.191538Z","iopub.execute_input":"2025-03-11T16:55:03.192026Z","iopub.status.idle":"2025-03-11T17:03:13.039227Z","shell.execute_reply.started":"2025-03-11T16:55:03.191989Z","shell.execute_reply":"2025-03-11T17:03:13.037939Z"}},"outputs":[{"name":"stdout","text":"Training CatBoost with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=50...\nCatBoost Classification Report (learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.48      0.38      0.42    359811\n           2       0.48      0.58      0.53    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4803\n\nMemory usage: 46.5%\nTraining CatBoost with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=100...\nCatBoost Classification Report (learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.46      0.37      0.41    359811\n           2       0.47      0.57      0.52    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.46    720000\nweighted avg       0.47      0.47      0.46    720000\n\nCatBoost Accuracy: 0.4678\n\nMemory usage: 47.0%\nTraining CatBoost with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=50...\nCatBoost Classification Report (learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.47      0.39      0.42    359811\n           2       0.48      0.57      0.52    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.47    720000\nweighted avg       0.48      0.48      0.47    720000\n\nCatBoost Accuracy: 0.4769\n\nMemory usage: 47.1%\nTraining CatBoost with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=100...\nCatBoost Classification Report (learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.39      0.42    359811\n           2       0.48      0.56      0.51    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4733\n\nMemory usage: 47.1%\nTraining CatBoost with learning_rate=0.05, iterations=100, depth=7, l2_leaf_reg=1, border_count=50...\nCatBoost Classification Report (learning_rate=0.05, iterations=100, depth=7, l2_leaf_reg=1, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.48      0.42      0.45    359811\n           2       0.49      0.55      0.52    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4836\n\nMemory usage: 47.0%\nTraining CatBoost with learning_rate=0.05, iterations=100, depth=7, l2_leaf_reg=1, border_count=100...\nCatBoost Classification Report (learning_rate=0.05, iterations=100, depth=7, l2_leaf_reg=1, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.47    720000\nweighted avg       0.48      0.48      0.47    720000\n\nCatBoost Accuracy: 0.4770\n\nMemory usage: 47.0%\nTraining CatBoost with learning_rate=0.05, iterations=100, depth=7, l2_leaf_reg=3, border_count=50...\nCatBoost Classification Report (learning_rate=0.05, iterations=100, depth=7, l2_leaf_reg=3, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4772\n\nMemory usage: 47.0%\nTraining CatBoost with learning_rate=0.05, iterations=100, depth=7, l2_leaf_reg=3, border_count=100...\nCatBoost Classification Report (learning_rate=0.05, iterations=100, depth=7, l2_leaf_reg=3, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.47      0.48      0.47    720000\nweighted avg       0.47      0.48      0.47    720000\n\nCatBoost Accuracy: 0.4754\n\nMemory usage: 47.0%\nTraining CatBoost with learning_rate=0.05, iterations=200, depth=5, l2_leaf_reg=1, border_count=50...\nCatBoost Classification Report (learning_rate=0.05, iterations=200, depth=5, l2_leaf_reg=1, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.54      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4732\n\nMemory usage: 47.1%\nTraining CatBoost with learning_rate=0.05, iterations=200, depth=5, l2_leaf_reg=1, border_count=100...\nCatBoost Classification Report (learning_rate=0.05, iterations=200, depth=5, l2_leaf_reg=1, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.54      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4725\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.05, iterations=200, depth=5, l2_leaf_reg=3, border_count=50...\nCatBoost Classification Report (learning_rate=0.05, iterations=200, depth=5, l2_leaf_reg=3, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.47    720000\nweighted avg       0.48      0.48      0.47    720000\n\nCatBoost Accuracy: 0.4771\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.05, iterations=200, depth=5, l2_leaf_reg=3, border_count=100...\nCatBoost Classification Report (learning_rate=0.05, iterations=200, depth=5, l2_leaf_reg=3, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4748\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.05, iterations=200, depth=7, l2_leaf_reg=1, border_count=50...\nCatBoost Classification Report (learning_rate=0.05, iterations=200, depth=7, l2_leaf_reg=1, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.47      0.43      0.45    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.47    720000\nweighted avg       0.48      0.48      0.47    720000\n\nCatBoost Accuracy: 0.4753\n\nMemory usage: 47.1%\nTraining CatBoost with learning_rate=0.05, iterations=200, depth=7, l2_leaf_reg=1, border_count=100...\nCatBoost Classification Report (learning_rate=0.05, iterations=200, depth=7, l2_leaf_reg=1, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.48      0.43      0.45    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4806\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.05, iterations=200, depth=7, l2_leaf_reg=3, border_count=50...\nCatBoost Classification Report (learning_rate=0.05, iterations=200, depth=7, l2_leaf_reg=3, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.47      0.43      0.45    359811\n           2       0.48      0.53      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4769\n\nMemory usage: 47.3%\nTraining CatBoost with learning_rate=0.05, iterations=200, depth=7, l2_leaf_reg=3, border_count=100...\nCatBoost Classification Report (learning_rate=0.05, iterations=200, depth=7, l2_leaf_reg=3, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.42      0.44    359811\n           2       0.48      0.53      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4722\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=100, depth=5, l2_leaf_reg=1, border_count=50...\nCatBoost Classification Report (learning_rate=0.1, iterations=100, depth=5, l2_leaf_reg=1, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.48      0.41      0.44    359811\n           2       0.48      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4790\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=100, depth=5, l2_leaf_reg=1, border_count=100...\nCatBoost Classification Report (learning_rate=0.1, iterations=100, depth=5, l2_leaf_reg=1, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.42      0.44    359811\n           2       0.48      0.53      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4747\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=100, depth=5, l2_leaf_reg=3, border_count=50...\nCatBoost Classification Report (learning_rate=0.1, iterations=100, depth=5, l2_leaf_reg=3, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.48      0.42      0.45    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4789\n\nMemory usage: 47.3%\nTraining CatBoost with learning_rate=0.1, iterations=100, depth=5, l2_leaf_reg=3, border_count=100...\nCatBoost Classification Report (learning_rate=0.1, iterations=100, depth=5, l2_leaf_reg=3, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.47      0.53      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4718\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=100, depth=7, l2_leaf_reg=1, border_count=50...\nCatBoost Classification Report (learning_rate=0.1, iterations=100, depth=7, l2_leaf_reg=1, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4776\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=100, depth=7, l2_leaf_reg=1, border_count=100...\nCatBoost Classification Report (learning_rate=0.1, iterations=100, depth=7, l2_leaf_reg=1, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.48      0.43      0.45    359811\n           2       0.48      0.53      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4801\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=100, depth=7, l2_leaf_reg=3, border_count=50...\nCatBoost Classification Report (learning_rate=0.1, iterations=100, depth=7, l2_leaf_reg=3, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.48      0.45      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4822\n\nMemory usage: 47.3%\nTraining CatBoost with learning_rate=0.1, iterations=100, depth=7, l2_leaf_reg=3, border_count=100...\nCatBoost Classification Report (learning_rate=0.1, iterations=100, depth=7, l2_leaf_reg=3, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.48      0.45      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4814\n\nMemory usage: 47.3%\nTraining CatBoost with learning_rate=0.1, iterations=200, depth=5, l2_leaf_reg=1, border_count=50...\nCatBoost Classification Report (learning_rate=0.1, iterations=200, depth=5, l2_leaf_reg=1, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.47      0.42      0.44    359811\n           2       0.47      0.52      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4711\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=200, depth=5, l2_leaf_reg=1, border_count=100...\nCatBoost Classification Report (learning_rate=0.1, iterations=200, depth=5, l2_leaf_reg=1, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.48      0.43      0.45    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4781\n\nMemory usage: 47.1%\nTraining CatBoost with learning_rate=0.1, iterations=200, depth=5, l2_leaf_reg=3, border_count=50...\nCatBoost Classification Report (learning_rate=0.1, iterations=200, depth=5, l2_leaf_reg=3, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.47      0.43      0.45    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4741\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=200, depth=5, l2_leaf_reg=3, border_count=100...\nCatBoost Classification Report (learning_rate=0.1, iterations=200, depth=5, l2_leaf_reg=3, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.47      0.43      0.45    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nCatBoost Accuracy: 0.4737\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=200, depth=7, l2_leaf_reg=1, border_count=50...\nCatBoost Classification Report (learning_rate=0.1, iterations=200, depth=7, l2_leaf_reg=1, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.48      0.45      0.46    359811\n           2       0.48      0.51      0.49    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4776\n\nMemory usage: 47.2%\nTraining CatBoost with learning_rate=0.1, iterations=200, depth=7, l2_leaf_reg=1, border_count=100...\nCatBoost Classification Report (learning_rate=0.1, iterations=200, depth=7, l2_leaf_reg=1, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.48      0.45      0.46    359811\n           2       0.48      0.51      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4792\n\nMemory usage: 47.1%\nTraining CatBoost with learning_rate=0.1, iterations=200, depth=7, l2_leaf_reg=3, border_count=50...\nCatBoost Classification Report (learning_rate=0.1, iterations=200, depth=7, l2_leaf_reg=3, border_count=50):\n              precision    recall  f1-score   support\n\n           1       0.48      0.45      0.46    359811\n           2       0.48      0.51      0.49    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4800\n\nMemory usage: 47.3%\nTraining CatBoost with learning_rate=0.1, iterations=200, depth=7, l2_leaf_reg=3, border_count=100...\nCatBoost Classification Report (learning_rate=0.1, iterations=200, depth=7, l2_leaf_reg=3, border_count=100):\n              precision    recall  f1-score   support\n\n           1       0.48      0.45      0.47    359811\n           2       0.48      0.51      0.49    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nCatBoost Accuracy: 0.4810\n\nMemory usage: 47.3%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score\nimport psutil\nimport numpy as np\nimport multiprocessing\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Ensure labels are numeric (assuming y_train, y_test are lists from FastText pipeline)\ny_train = np.array([int(label) for label in y_train])\ny_test = np.array([int(label) for label in y_test])\n\n# Conditional label adjustment (assuming original labels are 1-based [1, 2] as per CatBoost output)\ny_train_adjusted = y_train - 1 if np.min(y_train) > 0 else y_train\ny_test_adjusted = y_test - 1 if np.min(y_test) > 0 else y_test\n\n# Define expanded hyperparameter grid for RandomForest\nparam_grid = {\n    'n_estimators': [100, 200],  # More trees for better learning\n    'max_depth': [5, 10],  # Deeper trees\n    'min_samples_split': [2, 5]\n}\n\n# Stratified K-Fold Cross-Validation (increased to 5 folds for better validation)\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Use N_CORES from FastText code for consistency\nN_CORES = multiprocessing.cpu_count() - 1\n\n# Iterate over hyperparameters\nfor n_estimators in param_grid['n_estimators']:\n    for depth in param_grid['max_depth']:\n        for min_samples_split in param_grid['min_samples_split']:\n            print(f\"Training Random Forest with n_estimators={n_estimators}, depth={depth}, min_samples_split={min_samples_split}...\")\n            \n            # Initialize the RandomForest model\n            rf_classifier = RandomForestClassifier(\n                n_estimators=n_estimators,\n                max_depth=depth,\n                min_samples_split=min_samples_split,\n                n_jobs=N_CORES,  # Use same core count as FastText\n                random_state=42\n            )\n            \n            # Train on more data (e.g., 50,000 samples or full dataset if memory allows)\n            subset_size = min(50000, X_train_vectors.shape[0])  # Increased from 10,000\n            rf_classifier.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n            \n            # Make predictions\n            y_pred_rf = rf_classifier.predict(X_test_vectors)\n            if np.min(y_train) > 0:\n                y_pred_rf = y_pred_rf + 1\n            \n            # Performance evaluation\n            print(f\"Random Forest Classification Report (n_estimators={n_estimators}, depth={depth}, min_samples_split={min_samples_split}):\")\n            print(classification_report(y_test, y_pred_rf))\n            print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\\n\")\n            \n            # Check memory usage\n            check_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:03:50.161059Z","iopub.execute_input":"2025-03-11T17:03:50.161455Z","iopub.status.idle":"2025-03-11T17:07:18.517139Z","shell.execute_reply.started":"2025-03-11T17:03:50.161424Z","shell.execute_reply":"2025-03-11T17:07:18.515581Z"}},"outputs":[{"name":"stdout","text":"Training Random Forest with n_estimators=100, depth=5, min_samples_split=2...\nRandom Forest Classification Report (n_estimators=100, depth=5, min_samples_split=2):\n              precision    recall  f1-score   support\n\n           1       0.46      0.29      0.35    359811\n           2       0.48      0.66      0.56    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.47      0.48      0.46    720000\nweighted avg       0.47      0.48      0.46    720000\n\nRandom Forest Accuracy: 0.4761\n\nMemory usage: 47.4%\nTraining Random Forest with n_estimators=100, depth=5, min_samples_split=5...\nRandom Forest Classification Report (n_estimators=100, depth=5, min_samples_split=5):\n              precision    recall  f1-score   support\n\n           1       0.45      0.29      0.35    359811\n           2       0.48      0.66      0.55    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.45    720000\nweighted avg       0.47      0.47      0.45    720000\n\nRandom Forest Accuracy: 0.4716\n\nMemory usage: 47.5%\nTraining Random Forest with n_estimators=100, depth=10, min_samples_split=2...\nRandom Forest Classification Report (n_estimators=100, depth=10, min_samples_split=2):\n              precision    recall  f1-score   support\n\n           1       0.47      0.37      0.42    359811\n           2       0.48      0.58      0.53    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.47    720000\nweighted avg       0.48      0.48      0.47    720000\n\nRandom Forest Accuracy: 0.4788\n\nMemory usage: 47.6%\nTraining Random Forest with n_estimators=100, depth=10, min_samples_split=5...\nRandom Forest Classification Report (n_estimators=100, depth=10, min_samples_split=5):\n              precision    recall  f1-score   support\n\n           1       0.47      0.37      0.41    359811\n           2       0.48      0.58      0.53    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nRandom Forest Accuracy: 0.4738\n\nMemory usage: 47.7%\nTraining Random Forest with n_estimators=200, depth=5, min_samples_split=2...\nRandom Forest Classification Report (n_estimators=200, depth=5, min_samples_split=2):\n              precision    recall  f1-score   support\n\n           1       0.45      0.29      0.35    359811\n           2       0.48      0.64      0.55    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.46      0.47      0.45    720000\nweighted avg       0.46      0.47      0.45    720000\n\nRandom Forest Accuracy: 0.4667\n\nMemory usage: 47.6%\nTraining Random Forest with n_estimators=200, depth=5, min_samples_split=5...\nRandom Forest Classification Report (n_estimators=200, depth=5, min_samples_split=5):\n              precision    recall  f1-score   support\n\n           1       0.45      0.30      0.36    359811\n           2       0.47      0.63      0.54    360189\n\n    accuracy                           0.46    720000\n   macro avg       0.46      0.46      0.45    720000\nweighted avg       0.46      0.46      0.45    720000\n\nRandom Forest Accuracy: 0.4641\n\nMemory usage: 47.7%\nTraining Random Forest with n_estimators=200, depth=10, min_samples_split=2...\nRandom Forest Classification Report (n_estimators=200, depth=10, min_samples_split=2):\n              precision    recall  f1-score   support\n\n           1       0.46      0.37      0.41    359811\n           2       0.48      0.57      0.52    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nRandom Forest Accuracy: 0.4708\n\nMemory usage: 47.7%\nTraining Random Forest with n_estimators=200, depth=10, min_samples_split=5...\nRandom Forest Classification Report (n_estimators=200, depth=10, min_samples_split=5):\n              precision    recall  f1-score   support\n\n           1       0.46      0.36      0.40    359811\n           2       0.47      0.58      0.52    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.46    720000\nweighted avg       0.47      0.47      0.46    720000\n\nRandom Forest Accuracy: 0.4687\n\nMemory usage: 47.8%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# AdaBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score\nimport psutil\nimport numpy as np\nimport multiprocessing\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Ensure labels are numeric (assuming y_train, y_test are lists from FastText pipeline)\ny_train = np.array([int(label) for label in y_train])\ny_test = np.array([int(label) for label in y_test])\n\n# Conditional label adjustment (assuming original labels are 1-based [1, 2] as per previous outputs)\ny_train_adjusted = y_train - 1 if np.min(y_train) > 0 else y_train\ny_test_adjusted = y_test - 1 if np.min(y_test) > 0 else y_test\n\n# Define expanded hyperparameter grid for AdaBoost\nparam_grid = {\n    'n_estimators': [100, 200],  # More estimators for better boosting\n    'learning_rate': [0.1, 0.5]  # Higher learning rates for faster adaptation\n}\n\n# Stratified K-Fold Cross-Validation (increased to 5 folds for better validation)\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Use N_CORES from FastText code for consistency (though AdaBoost doesn’t use n_jobs directly)\nN_CORES = multiprocessing.cpu_count() - 1\n\n# Iterate over hyperparameters\nfor n_estimators in param_grid['n_estimators']:\n    for learning_rate in param_grid['learning_rate']:\n        print(f\"Training AdaBoost with n_estimators={n_estimators}, learning_rate={learning_rate}...\")\n        \n        # Initialize the AdaBoost model\n        adaboost_classifier = AdaBoostClassifier(\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=42\n        )\n        \n        # Train on more data (e.g., 50,000 samples or full dataset if memory allows)\n        subset_size = min(50000, X_train_vectors.shape[0])  # Increased from 10,000\n        adaboost_classifier.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n        \n        # Make predictions\n        y_pred_adaboost = adaboost_classifier.predict(X_test_vectors)\n        if np.min(y_train) > 0:\n            y_pred_adaboost = y_pred_adaboost + 1\n        \n        # Performance evaluation\n        print(f\"AdaBoost Classification Report (n_estimators={n_estimators}, learning_rate={learning_rate}):\")\n        print(classification_report(y_test, y_pred_adaboost))\n        print(f\"AdaBoost Accuracy: {accuracy_score(y_test, y_pred_adaboost):.4f}\\n\")\n        \n        # Check memory usage\n        check_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:09:02.412901Z","iopub.execute_input":"2025-03-11T17:09:02.413333Z","iopub.status.idle":"2025-03-11T17:15:51.310601Z","shell.execute_reply.started":"2025-03-11T17:09:02.413299Z","shell.execute_reply":"2025-03-11T17:15:51.308634Z"}},"outputs":[{"name":"stdout","text":"Training AdaBoost with n_estimators=100, learning_rate=0.1...\nAdaBoost Classification Report (n_estimators=100, learning_rate=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.32      0.38    359811\n           2       0.49      0.65      0.56    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.47    720000\nweighted avg       0.49      0.49      0.47    720000\n\nAdaBoost Accuracy: 0.4871\n\nMemory usage: 47.9%\nTraining AdaBoost with n_estimators=100, learning_rate=0.5...\nAdaBoost Classification Report (n_estimators=100, learning_rate=0.5):\n              precision    recall  f1-score   support\n\n           1       0.46      0.41      0.43    359811\n           2       0.47      0.52      0.49    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.46      0.47      0.46    720000\nweighted avg       0.46      0.47      0.46    720000\n\nAdaBoost Accuracy: 0.4653\n\nMemory usage: 47.9%\nTraining AdaBoost with n_estimators=200, learning_rate=0.1...\nAdaBoost Classification Report (n_estimators=200, learning_rate=0.1):\n              precision    recall  f1-score   support\n\n           1       0.47      0.36      0.41    359811\n           2       0.48      0.60      0.53    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.47    720000\nweighted avg       0.48      0.48      0.47    720000\n\nAdaBoost Accuracy: 0.4791\n\nMemory usage: 47.9%\nTraining AdaBoost with n_estimators=200, learning_rate=0.5...\nAdaBoost Classification Report (n_estimators=200, learning_rate=0.5):\n              precision    recall  f1-score   support\n\n           1       0.47      0.43      0.45    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nAdaBoost Accuracy: 0.4744\n\nMemory usage: 47.8%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Naive Bayes and SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report, accuracy_score\nimport numpy as np\nimport psutil\nimport multiprocessing\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Ensure labels are numeric (assuming y_train, y_test are lists from FastText pipeline)\ny_train = np.array([int(label) for label in y_train])\ny_test = np.array([int(label) for label in y_test])\n\n# Conditional label adjustment (assuming original labels are 1-based [1, 2] as per previous outputs)\ny_train_adjusted = y_train - 1 if np.min(y_train) > 0 else y_train\ny_test_adjusted = y_test - 1 if np.min(y_test) > 0 else y_test\n\n# Use N_CORES from FastText code for consistency (not used here, kept for reference)\nN_CORES = multiprocessing.cpu_count() - 1\n\n# Naive Bayes (GaussianNB) training\nprint(\"Training Gaussian Naive Bayes...\")\nnb_classifier = GaussianNB()\n# Train on a subset for efficiency\nsubset_size = min(50000, X_train_vectors.shape[0])\nnb_classifier.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n\n# Predict\ny_pred_nb = nb_classifier.predict(X_test_vectors)\nif np.min(y_train) > 0:\n    y_pred_nb = y_pred_nb + 1\n\n# Performance evaluation\nprint(\"Gaussian Naive Bayes Classification Report:\")\nprint(classification_report(y_test, y_pred_nb))\nprint(f\"Gaussian Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\\n\")\ncheck_memory()\n\n# SVM hyperparameter tuning\nc_values = [0.1, 0.5, 1.0, 2.0]\nfor c_value in c_values:\n    print(f\"Training SVM with C={c_value}...\")\n    svm_classifier = LinearSVC(\n        C=c_value,\n        max_iter=5000,\n        random_state=42,\n        dual=False  # Faster for dense data with n_samples > n_features\n    )\n    \n    # Train on a subset for efficiency\n    subset_size = min(50000, X_train_vectors.shape[0])\n    svm_classifier.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n    \n    # Predict\n    y_pred_svm = svm_classifier.predict(X_test_vectors)\n    if np.min(y_train) > 0:\n        y_pred_svm = y_pred_svm + 1\n    \n    # Performance evaluation\n    print(f\"SVM Classification Report (C={c_value}):\")\n    print(classification_report(y_test, y_pred_svm))\n    print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\\n\")\n    check_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:17:19.533450Z","iopub.execute_input":"2025-03-11T17:17:19.533844Z","iopub.status.idle":"2025-03-11T17:17:33.656634Z","shell.execute_reply.started":"2025-03-11T17:17:19.533811Z","shell.execute_reply":"2025-03-11T17:17:33.655524Z"}},"outputs":[{"name":"stdout","text":"Training Gaussian Naive Bayes...\nGaussian Naive Bayes Classification Report:\n              precision    recall  f1-score   support\n\n           1       0.46      0.53      0.49    359811\n           2       0.44      0.38      0.41    360189\n\n    accuracy                           0.45    720000\n   macro avg       0.45      0.45      0.45    720000\nweighted avg       0.45      0.45      0.45    720000\n\nGaussian Naive Bayes Accuracy: 0.4525\n\nMemory usage: 47.8%\nTraining SVM with C=0.1...\nSVM Classification Report (C=0.1):\n              precision    recall  f1-score   support\n\n           1       0.46      0.40      0.43    359811\n           2       0.47      0.53      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.46    720000\nweighted avg       0.47      0.47      0.46    720000\n\nSVM Accuracy: 0.4667\n\nMemory usage: 47.7%\nTraining SVM with C=0.5...\nSVM Classification Report (C=0.5):\n              precision    recall  f1-score   support\n\n           1       0.46      0.40      0.43    359811\n           2       0.47      0.53      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.46    720000\nweighted avg       0.47      0.47      0.46    720000\n\nSVM Accuracy: 0.4667\n\nMemory usage: 47.8%\nTraining SVM with C=1.0...\nSVM Classification Report (C=1.0):\n              precision    recall  f1-score   support\n\n           1       0.46      0.40      0.43    359811\n           2       0.47      0.53      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.46    720000\nweighted avg       0.47      0.47      0.46    720000\n\nSVM Accuracy: 0.4666\n\nMemory usage: 47.8%\nTraining SVM with C=2.0...\nSVM Classification Report (C=2.0):\n              precision    recall  f1-score   support\n\n           1       0.46      0.40      0.43    359811\n           2       0.47      0.53      0.50    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.46    720000\nweighted avg       0.47      0.47      0.46    720000\n\nSVM Accuracy: 0.4666\n\nMemory usage: 47.8%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# XG Boost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport psutil\nimport multiprocessing\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Ensure labels are numeric (assuming y_train, y_test are lists from FastText pipeline)\ny_train = np.array([int(label) for label in y_train])\ny_test = np.array([int(label) for label in y_test])\n\n# Conditional label adjustment (assuming original labels are 1-based [1, 2] as per previous outputs)\ny_train_adjusted = y_train - 1 if np.min(y_train) > 0 else y_train\ny_test_adjusted = y_test - 1 if np.min(y_test) > 0 else y_test\n\n# Define hyperparameter grid for XGBoost\nparam_grid = {\n    'learning_rate': [0.05, 0.1],  # Focus on moderate to higher rates\n    'n_estimators': [100, 200],    # More trees for better learning\n    'max_depth': [3, 5],           # Moderate depths\n    'min_child_weight': [1, 5],    # Control overfitting\n    'gamma': [0, 0.1]             # Minimum loss reduction\n}\n\n# Stratified K-Fold Cross-Validation (increased to 5 folds for better validation)\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Use N_CORES from FastText code for consistency\nN_CORES = multiprocessing.cpu_count() - 1\n\n# Iterate over hyperparameters\nfor lr in param_grid['learning_rate']:\n    for n_est in param_grid['n_estimators']:\n        for max_depth in param_grid['max_depth']:\n            for min_child_weight in param_grid['min_child_weight']:\n                for gamma in param_grid['gamma']:\n                    print(f\"Training XGBoost with learning_rate={lr}, n_estimators={n_est}, max_depth={max_depth}, min_child_weight={min_child_weight}, gamma={gamma}...\")\n                    \n                    # Initialize the XGBoost model\n                    xgb_classifier = XGBClassifier(\n                        learning_rate=lr,\n                        n_estimators=n_est,\n                        max_depth=max_depth,\n                        min_child_weight=min_child_weight,\n                        gamma=gamma,\n                        use_label_encoder=False,  # Avoid deprecation warning\n                        eval_metric='logloss',    # For binary classification\n                        tree_method='hist',       # Faster histogram-based method\n                        n_jobs=N_CORES,           # Parallelize across cores\n                        random_state=42\n                    )\n                    \n                    # Train on more data (e.g., 50,000 samples or full dataset if memory allows)\n                    subset_size = min(50000, X_train_vectors.shape[0])  # Increased from 10,000\n                    xgb_classifier.fit(X_train_vectors[:subset_size], y_train_adjusted[:subset_size])\n                    \n                    # Make predictions\n                    y_pred_xgb = xgb_classifier.predict(X_test_vectors)\n                    if np.min(y_train) > 0:\n                        y_pred_xgb = y_pred_xgb + 1\n                    \n                    # Performance evaluation\n                    print(f\"XGBoost Classification Report (learning_rate={lr}, n_estimators={n_est}, max_depth={max_depth}, min_child_weight={min_child_weight}, gamma={gamma}):\")\n                    print(classification_report(y_test, y_pred_xgb))\n                    print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\\n\")\n                    \n                    # Check memory usage\n                    check_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:19:13.437996Z","iopub.execute_input":"2025-03-11T17:19:13.438394Z","iopub.status.idle":"2025-03-11T17:21:25.360620Z","shell.execute_reply.started":"2025-03-11T17:19:13.438365Z","shell.execute_reply":"2025-03-11T17:21:25.359508Z"}},"outputs":[{"name":"stdout","text":"Training XGBoost with learning_rate=0.05, n_estimators=100, max_depth=3, min_child_weight=1, gamma=0...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=100, max_depth=3, min_child_weight=1, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.39      0.43    359811\n           2       0.48      0.57      0.53    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4805\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=100, max_depth=3, min_child_weight=1, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=100, max_depth=3, min_child_weight=1, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.39      0.43    359811\n           2       0.48      0.57      0.53    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4805\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=100, max_depth=3, min_child_weight=5, gamma=0...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=100, max_depth=3, min_child_weight=5, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.47      0.39      0.43    359811\n           2       0.48      0.56      0.51    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nXGBoost Accuracy: 0.4737\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=100, max_depth=3, min_child_weight=5, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=100, max_depth=3, min_child_weight=5, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.47      0.39      0.43    359811\n           2       0.48      0.56      0.51    360189\n\n    accuracy                           0.47    720000\n   macro avg       0.47      0.47      0.47    720000\nweighted avg       0.47      0.47      0.47    720000\n\nXGBoost Accuracy: 0.4737\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.42      0.45    359811\n           2       0.48      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4821\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.42      0.45    359811\n           2       0.49      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4830\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=100, max_depth=5, min_child_weight=5, gamma=0...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=100, max_depth=5, min_child_weight=5, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4787\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=100, max_depth=5, min_child_weight=5, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=100, max_depth=5, min_child_weight=5, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.47      0.41      0.44    359811\n           2       0.48      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4787\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=200, max_depth=3, min_child_weight=1, gamma=0...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=200, max_depth=3, min_child_weight=1, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.42      0.45    359811\n           2       0.49      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4829\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=200, max_depth=3, min_child_weight=1, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=200, max_depth=3, min_child_weight=1, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.42      0.45    359811\n           2       0.49      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4829\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=200, max_depth=3, min_child_weight=5, gamma=0...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=200, max_depth=3, min_child_weight=5, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.41      0.44    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4791\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=200, max_depth=3, min_child_weight=5, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=200, max_depth=3, min_child_weight=5, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.41      0.44    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4791\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=200, max_depth=5, min_child_weight=1, gamma=0...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=200, max_depth=5, min_child_weight=1, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4811\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=200, max_depth=5, min_child_weight=1, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=200, max_depth=5, min_child_weight=1, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.48      0.53      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4830\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=200, max_depth=5, min_child_weight=5, gamma=0...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=200, max_depth=5, min_child_weight=5, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.43      0.46    359811\n           2       0.48      0.53      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4811\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.05, n_estimators=200, max_depth=5, min_child_weight=5, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.05, n_estimators=200, max_depth=5, min_child_weight=5, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.43      0.46    359811\n           2       0.48      0.53      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4811\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=1, gamma=0...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=1, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.47      0.42      0.45    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4780\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=1, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=1, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.47      0.42      0.45    359811\n           2       0.48      0.54      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4780\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=5, gamma=0...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=5, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.42      0.45    359811\n           2       0.49      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4846\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=5, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=5, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.42      0.45    359811\n           2       0.49      0.55      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4846\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4829\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.49      0.53      0.51    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4848\n\nMemory usage: 48.0%\nTraining XGBoost with learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=5, gamma=0...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=5, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4830\n\nMemory usage: 48.0%\nTraining XGBoost with learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=5, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=5, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4830\n\nMemory usage: 48.0%\nTraining XGBoost with learning_rate=0.1, n_estimators=200, max_depth=3, min_child_weight=1, gamma=0...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=200, max_depth=3, min_child_weight=1, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4825\n\nMemory usage: 48.0%\nTraining XGBoost with learning_rate=0.1, n_estimators=200, max_depth=3, min_child_weight=1, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=200, max_depth=3, min_child_weight=1, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.48      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4825\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=200, max_depth=3, min_child_weight=5, gamma=0...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=200, max_depth=3, min_child_weight=5, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.49      0.53      0.51    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.48    720000\nweighted avg       0.49      0.49      0.48    720000\n\nXGBoost Accuracy: 0.4855\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=200, max_depth=3, min_child_weight=5, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=200, max_depth=3, min_child_weight=5, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.44      0.46    359811\n           2       0.49      0.53      0.51    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.48    720000\nweighted avg       0.49      0.49      0.48    720000\n\nXGBoost Accuracy: 0.4855\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=200, max_depth=5, min_child_weight=1, gamma=0...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=200, max_depth=5, min_child_weight=1, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.46      0.47    359811\n           2       0.49      0.51      0.50    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.49    720000\nweighted avg       0.49      0.49      0.49    720000\n\nXGBoost Accuracy: 0.4854\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=200, max_depth=5, min_child_weight=1, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=200, max_depth=5, min_child_weight=1, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.49      0.46      0.47    359811\n           2       0.49      0.52      0.50    360189\n\n    accuracy                           0.49    720000\n   macro avg       0.49      0.49      0.49    720000\nweighted avg       0.49      0.49      0.49    720000\n\nXGBoost Accuracy: 0.4862\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=200, max_depth=5, min_child_weight=5, gamma=0...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=200, max_depth=5, min_child_weight=5, gamma=0):\n              precision    recall  f1-score   support\n\n           1       0.48      0.45      0.47    359811\n           2       0.49      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4842\n\nMemory usage: 47.9%\nTraining XGBoost with learning_rate=0.1, n_estimators=200, max_depth=5, min_child_weight=5, gamma=0.1...\nXGBoost Classification Report (learning_rate=0.1, n_estimators=200, max_depth=5, min_child_weight=5, gamma=0.1):\n              precision    recall  f1-score   support\n\n           1       0.48      0.45      0.47    359811\n           2       0.49      0.52      0.50    360189\n\n    accuracy                           0.48    720000\n   macro avg       0.48      0.48      0.48    720000\nweighted avg       0.48      0.48      0.48    720000\n\nXGBoost Accuracy: 0.4842\n\nMemory usage: 47.9%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MLP Classifier","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}