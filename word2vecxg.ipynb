{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":800230,"sourceType":"datasetVersion","datasetId":1305},{"sourceId":10479620,"sourceType":"datasetVersion","datasetId":6489054}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import fireducks.pandas as pd\nimport os\n\n# Path to the extracted chunk files (from your Kaggle dataset structure)\nextracted_chunks_path = \"/kaggle/input/processed-chunks-1\"  # Adjust if the path differs\n\n# Combine all chunk files\nall_chunks = []\nfor file_name in sorted(os.listdir(extracted_chunks_path)):  # Ensure files are combined in order\n    if file_name.startswith(\"processed_chunk_\") and file_name.endswith(\".csv\"):\n        file_path = os.path.join(extracted_chunks_path, file_name)\n        print(f\"Loading {file_name}...\")\n        chunk = pd.read_csv(file_path)\n        all_chunks.append(chunk)\n\n# Concatenate all chunks into a single DataFrame\ncombined_data = pd.concat(all_chunks, ignore_index=True)\n\n# Display combined data info\nprint(\"Combined data shape:\", combined_data.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:38:51.370950Z","iopub.execute_input":"2025-02-01T15:38:51.371280Z","iopub.status.idle":"2025-02-01T15:39:13.263583Z","shell.execute_reply.started":"2025-02-01T15:38:51.371230Z","shell.execute_reply":"2025-02-01T15:39:13.262842Z"}},"outputs":[{"name":"stdout","text":"Loading processed_chunk_0_50000.csv...\nLoading processed_chunk_1000000_1050000.csv...\nLoading processed_chunk_100000_150000.csv...\nLoading processed_chunk_1050000_1100000.csv...\nLoading processed_chunk_1100000_1150000.csv...\nLoading processed_chunk_1150000_1200000.csv...\nLoading processed_chunk_1200000_1250000.csv...\nLoading processed_chunk_1250000_1300000.csv...\nLoading processed_chunk_1300000_1350000.csv...\nLoading processed_chunk_1350000_1400000.csv...\nLoading processed_chunk_1400000_1450000.csv...\nLoading processed_chunk_1450000_1500000.csv...\nLoading processed_chunk_1500000_1550000.csv...\nLoading processed_chunk_150000_200000.csv...\nLoading processed_chunk_1550000_1600000.csv...\nLoading processed_chunk_1600000_1650000.csv...\nLoading processed_chunk_1650000_1700000.csv...\nLoading processed_chunk_1700000_1750000.csv...\nLoading processed_chunk_1750000_1800000.csv...\nLoading processed_chunk_1800000_1850000.csv...\nLoading processed_chunk_1850000_1900000.csv...\nLoading processed_chunk_1900000_1950000.csv...\nLoading processed_chunk_1950000_2000000.csv...\nLoading processed_chunk_2000000_2050000.csv...\nLoading processed_chunk_200000_250000.csv...\nLoading processed_chunk_2050000_2100000.csv...\nLoading processed_chunk_2100000_2150000.csv...\nLoading processed_chunk_2150000_2200000.csv...\nLoading processed_chunk_2200000_2250000.csv...\nLoading processed_chunk_2250000_2300000.csv...\nLoading processed_chunk_2300000_2350000.csv...\nLoading processed_chunk_2350000_2400000.csv...\nLoading processed_chunk_2400000_2450000.csv...\nLoading processed_chunk_2450000_2500000.csv...\nLoading processed_chunk_2500000_2550000.csv...\nLoading processed_chunk_250000_300000.csv...\nLoading processed_chunk_2550000_2600000.csv...\nLoading processed_chunk_2600000_2650000.csv...\nLoading processed_chunk_2650000_2700000.csv...\nLoading processed_chunk_2700000_2750000.csv...\nLoading processed_chunk_2750000_2800000.csv...\nLoading processed_chunk_2800000_2850000.csv...\nLoading processed_chunk_2850000_2900000.csv...\nLoading processed_chunk_2900000_2950000.csv...\nLoading processed_chunk_2950000_3000000.csv...\nLoading processed_chunk_3000000_3050000.csv...\nLoading processed_chunk_300000_350000.csv...\nLoading processed_chunk_3050000_3100000.csv...\nLoading processed_chunk_3100000_3150000.csv...\nLoading processed_chunk_3150000_3200000.csv...\nLoading processed_chunk_3200000_3250000.csv...\nLoading processed_chunk_3250000_3300000.csv...\nLoading processed_chunk_3300000_3350000.csv...\nLoading processed_chunk_3350000_3400000.csv...\nLoading processed_chunk_3400000_3450000.csv...\nLoading processed_chunk_3450000_3500000.csv...\nLoading processed_chunk_3500000_3550000.csv...\nLoading processed_chunk_350000_400000.csv...\nLoading processed_chunk_3550000_3600000.csv...\nLoading processed_chunk_400000_450000.csv...\nLoading processed_chunk_450000_500000.csv...\nLoading processed_chunk_500000_550000.csv...\nLoading processed_chunk_50000_100000.csv...\nLoading processed_chunk_550000_600000.csv...\nLoading processed_chunk_600000_650000.csv...\nLoading processed_chunk_650000_700000.csv...\nLoading processed_chunk_700000_750000.csv...\nLoading processed_chunk_750000_800000.csv...\nLoading processed_chunk_800000_850000.csv...\nLoading processed_chunk_850000_900000.csv...\nLoading processed_chunk_900000_950000.csv...\nLoading processed_chunk_950000_1000000.csv...\nCombined data shape: (3600000, 3)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Save the combined dataset as a CSV for future use\ncombined_data_path = \"/kaggle/working/combined_processed_data.csv\"\ncombined_data.to_csv(combined_data_path, index=False)\nprint(f\"Combined data saved at: {combined_data_path}\")\n# Load the saved combined dataset\ncombined_data = pd.read_csv(\"/kaggle/working/combined_processed_data.csv\")\n\n# Check the dataset structure\nprint(combined_data.info())\nprint(combined_data.head())\n# Check label distribution\nprint(combined_data['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:39:13.264570Z","iopub.execute_input":"2025-02-01T15:39:13.264951Z","iopub.status.idle":"2025-02-01T15:39:27.941283Z","shell.execute_reply.started":"2025-02-01T15:39:13.264927Z","shell.execute_reply":"2025-02-01T15:39:27.940536Z"}},"outputs":[{"name":"stdout","text":"Combined data saved at: /kaggle/working/combined_processed_data.csv\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3600000 entries, 0 to 3599999\nData columns (total 3 columns):\n #   Column          Dtype \n---  ------          ----- \n 0   review          object\n 1   label           int64 \n 2   cleaned_review  object\ndtypes: int64(1), object(2)\nmemory usage: 82.4+ MB\nNone\n                                              review  label  \\\n0  Stuning even for the non-gamer: This sound tra...      2   \n1  The best soundtrack ever to anything.: I'm rea...      2   \n2  Amazing!: This soundtrack is my favorite music...      2   \n3  Excellent Soundtrack: I truly like this soundt...      2   \n4  Remember, Pull Your Jaw Off The Floor After He...      2   \n\n                                      cleaned_review  \n0  stun non gamer sound track beautiful paint sen...  \n1  good soundtrack read lot review say good game ...  \n2  amazing soundtrack favorite music time hand in...  \n3  excellent soundtrack truly like soundtrack enj...  \n4  remember pull Jaw Floor hear play game know di...  \nlabel\n2    1800000\n1    1800000\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX = combined_data['cleaned_review']  # Features (cleaned reviews)\ny = combined_data['label']           # Labels (1 for neutral, 2 for positive)\n\n# Perform train-test split (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Testing samples: {X_test.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:39:27.942340Z","iopub.execute_input":"2025-02-01T15:39:27.942627Z","iopub.status.idle":"2025-02-01T15:39:32.510493Z","shell.execute_reply.started":"2025-02-01T15:39:27.942604Z","shell.execute_reply":"2025-02-01T15:39:32.509740Z"}},"outputs":[{"name":"stdout","text":"Training samples: 2880000\nTesting samples: 720000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Transformer model","metadata":{}},{"cell_type":"code","source":"# from gensim.models import Word2Vec, FastText\n# from sklearn.model_selection import train_test_split\n# import numpy as np\n# import multiprocessing\n# import gc\n\n# # Ensure all reviews are strings before splitting and handle missing values\n# combined_data['cleaned_review'] = combined_data['cleaned_review'].fillna('').astype(str)\n# X = combined_data['cleaned_review']\n# y = combined_data['label']\n\n# # Reduce dataset size to prevent memory issues\n# sample_size = min(500000, len(X))  # Limit to 500k samples if dataset is larger\n# X_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, random_state=42)\n\n# # Split into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42)\n\n# # Tokenize the text data\n# sentences_train = [review.split() for review in X_train]\n# sentences_test = [review.split() for review in X_test]\n\n# # Optimize training parameters for speed and memory\n# num_workers = multiprocessing.cpu_count() // 2  # Use half of available CPU cores\n# vector_size = 50  # Reduce size to optimize speed and memory\n# min_count = 5  # Ignore words that appear less frequently\n# window = 4  # Slightly smaller context window\n# epochs = 3  # Reduce epochs to save memory\n\n# # Train Word2Vec Model\n# w2v_model = Word2Vec(sentences_train, vector_size=vector_size, window=window, min_count=min_count, workers=num_workers)\n# w2v_model.train(sentences_train, total_examples=len(sentences_train), epochs=epochs)\n\n# # Train FastText Model\n# ft_model = FastText(sentences_train, vector_size=vector_size, window=window, min_count=min_count, workers=num_workers)\n# ft_model.train(sentences_train, total_examples=len(sentences_train), epochs=epochs)\n\n# # Function to convert reviews to vectors by averaging word embeddings\n# def get_avg_word_vector(words, model, vector_size):\n#     vectors = [model.wv[word] for word in words if word in model.wv]\n#     return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n\n# # Convert train and test sets into vectors\n# X_train_w2v = np.array([get_avg_word_vector(review, w2v_model, vector_size) for review in sentences_train], dtype=np.float32)\n# X_test_w2v = np.array([get_avg_word_vector(review, w2v_model, vector_size) for review in sentences_test], dtype=np.float32)\n\n# X_train_ft = np.array([get_avg_word_vector(review, ft_model, vector_size) for review in sentences_train], dtype=np.float32)\n# X_test_ft = np.array([get_avg_word_vector(review, ft_model, vector_size) for review in sentences_test], dtype=np.float32)\n\n# # Free memory\n# gc.collect()\n\n# print(\"Feature engineering complete.\")\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport gc\n\n# Load SBERT model (optimized for speed and accuracy)\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Small, fast model\n\n# Ensure all reviews are strings before splitting and handle missing values\ncombined_data['cleaned_review'] = combined_data['cleaned_review'].fillna('').astype(str)\nX = combined_data['cleaned_review']\ny = combined_data['label']\n\n# Reduce dataset size to prevent memory issues\nsample_size = min(500000, len(X))  # Limit to 500k samples if dataset is larger\nX_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, random_state=42)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42)\n\n# Compute SBERT embeddings for train and test sets\nX_train_sbert = np.array(sbert_model.encode(X_train.tolist(), batch_size=32, convert_to_numpy=True, normalize_embeddings=True))\nX_test_sbert = np.array(sbert_model.encode(X_test.tolist(), batch_size=32, convert_to_numpy=True, normalize_embeddings=True))\n\n# Free memory\ngc.collect()\n\nprint(\"SBERT feature engineering complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:39:40.120941Z","iopub.execute_input":"2025-02-01T15:39:40.121423Z","iopub.status.idle":"2025-02-01T15:44:32.351295Z","shell.execute_reply.started":"2025-02-01T15:39:40.121392Z","shell.execute_reply":"2025-02-01T15:44:32.350560Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7085a05d99ae4aea84723abf49515d00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9772402f965489fb986de6b01bfd41f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bf1db3faea4b1ca4c8a6a0678b76ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e087509618fc440eb3685bc61d89e31c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bada72003ab4188973a382c2b95f3a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ce56eec56344e2b126cbf66f39a1d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"517d483a326244cd8ed40b277441e5fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d93ea28543b345b1a3067201ddc4aac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1fedff51514726ab32e32c8939231b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f82175760964dbd8acfc5eb19047f33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d1c5fddb6b94c8387a415bfc5284877"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/12500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b283170382f7472ebad46317e0302527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6e488b57254148b6e935f34178d4d9"}},"metadata":{}},{"name":"stdout","text":"SBERT feature engineering complete.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# XG Boost","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score\nimport psutil\nimport numpy as np\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Adjust target labels to be 0-based\ny_train_adjusted = y_train - 1\ny_test_adjusted = y_test - 1\n\n# Convert labels to numpy arrays\ny_train_adjusted = np.array(y_train_adjusted)\ny_test_adjusted = np.array(y_test_adjusted)\n\n# Define hyperparameter grid for LightGBM (with smaller iterations)\nlearning_rates = [0.01, 0.05]\niterations_values = [50, 100]  # Reduced iterations\ndepth_values = [3, 5]\nnum_leaves_values = [15, 31]\n\n# Cross-validation setup (reduced folds to save memory)\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 3 folds\n\n# Iterate over hyperparameters\nfor lr in learning_rates:\n    for iterations in iterations_values:\n        for depth in depth_values:\n            for num_leaves in num_leaves_values:\n                print(f\"Training LightGBM classifier with learning_rate={lr}, iterations={iterations}, depth={depth}, num_leaves={num_leaves}...\")\n\n                # Set up the LightGBM model with the specified hyperparameters\n                lgbm_classifier = lgb.LGBMClassifier(\n                    learning_rate=lr,\n                    n_estimators=iterations,\n                    max_depth=depth,\n                    num_leaves=num_leaves,\n                    verbose=-1,\n                    n_jobs=-1  # Use all available CPU cores\n                )\n\n                # Cross-validation setup\n                lgbm_classifier.fit(X_train_sbert[:10000], y_train_adjusted[:10000])  # Use a smaller subset\n                y_pred_lgbm = lgbm_classifier.predict(X_test_sbert)\n\n                # Adjust predictions back to the original label scale for reporting\n                y_pred_lgbm_original = y_pred_lgbm + 1\n\n                # Performance reporting\n                print(f\"LightGBM Classification Report for learning_rate={lr}, iterations={iterations}, depth={depth}, num_leaves={num_leaves}:\")\n                print(classification_report(y_test, y_pred_lgbm_original))\n                accuracy = accuracy_score(y_test, y_pred_lgbm_original)\n                print(f\"LightGBM Accuracy: {accuracy:.4f}\\n\")\n\n                # Check memory usage after each iteration\n                check_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:45:19.760738Z","iopub.execute_input":"2025-02-01T15:45:19.761437Z","iopub.status.idle":"2025-02-01T15:45:49.384989Z","shell.execute_reply.started":"2025-02-01T15:45:19.761405Z","shell.execute_reply":"2025-02-01T15:45:49.384038Z"}},"outputs":[{"name":"stdout","text":"Training LightGBM classifier with learning_rate=0.01, iterations=50, depth=3, num_leaves=15...\nLightGBM Classification Report for learning_rate=0.01, iterations=50, depth=3, num_leaves=15:\n              precision    recall  f1-score   support\n\n           1       0.68      0.66      0.67     49906\n           2       0.67      0.69      0.68     50094\n\n    accuracy                           0.68    100000\n   macro avg       0.68      0.68      0.68    100000\nweighted avg       0.68      0.68      0.68    100000\n\nLightGBM Accuracy: 0.6778\n\nMemory usage: 38.2%\nTraining LightGBM classifier with learning_rate=0.01, iterations=50, depth=3, num_leaves=31...\nLightGBM Classification Report for learning_rate=0.01, iterations=50, depth=3, num_leaves=31:\n              precision    recall  f1-score   support\n\n           1       0.68      0.66      0.67     49906\n           2       0.67      0.69      0.68     50094\n\n    accuracy                           0.68    100000\n   macro avg       0.68      0.68      0.68    100000\nweighted avg       0.68      0.68      0.68    100000\n\nLightGBM Accuracy: 0.6778\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.01, iterations=50, depth=5, num_leaves=15...\nLightGBM Classification Report for learning_rate=0.01, iterations=50, depth=5, num_leaves=15:\n              precision    recall  f1-score   support\n\n           1       0.71      0.66      0.69     49906\n           2       0.69      0.73      0.71     50094\n\n    accuracy                           0.70    100000\n   macro avg       0.70      0.70      0.70    100000\nweighted avg       0.70      0.70      0.70    100000\n\nLightGBM Accuracy: 0.6975\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.01, iterations=50, depth=5, num_leaves=31...\nLightGBM Classification Report for learning_rate=0.01, iterations=50, depth=5, num_leaves=31:\n              precision    recall  f1-score   support\n\n           1       0.72      0.69      0.70     49906\n           2       0.70      0.74      0.72     50094\n\n    accuracy                           0.71    100000\n   macro avg       0.71      0.71      0.71    100000\nweighted avg       0.71      0.71      0.71    100000\n\nLightGBM Accuracy: 0.7130\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.01, iterations=100, depth=3, num_leaves=15...\nLightGBM Classification Report for learning_rate=0.01, iterations=100, depth=3, num_leaves=15:\n              precision    recall  f1-score   support\n\n           1       0.71      0.70      0.71     49906\n           2       0.71      0.72      0.71     50094\n\n    accuracy                           0.71    100000\n   macro avg       0.71      0.71      0.71    100000\nweighted avg       0.71      0.71      0.71    100000\n\nLightGBM Accuracy: 0.7090\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.01, iterations=100, depth=3, num_leaves=31...\nLightGBM Classification Report for learning_rate=0.01, iterations=100, depth=3, num_leaves=31:\n              precision    recall  f1-score   support\n\n           1       0.71      0.70      0.71     49906\n           2       0.71      0.72      0.71     50094\n\n    accuracy                           0.71    100000\n   macro avg       0.71      0.71      0.71    100000\nweighted avg       0.71      0.71      0.71    100000\n\nLightGBM Accuracy: 0.7090\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.01, iterations=100, depth=5, num_leaves=15...\nLightGBM Classification Report for learning_rate=0.01, iterations=100, depth=5, num_leaves=15:\n              precision    recall  f1-score   support\n\n           1       0.74      0.70      0.72     49906\n           2       0.72      0.75      0.73     50094\n\n    accuracy                           0.73    100000\n   macro avg       0.73      0.73      0.73    100000\nweighted avg       0.73      0.73      0.73    100000\n\nLightGBM Accuracy: 0.7267\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.01, iterations=100, depth=5, num_leaves=31...\nLightGBM Classification Report for learning_rate=0.01, iterations=100, depth=5, num_leaves=31:\n              precision    recall  f1-score   support\n\n           1       0.75      0.72      0.73     49906\n           2       0.73      0.76      0.74     50094\n\n    accuracy                           0.74    100000\n   macro avg       0.74      0.74      0.74    100000\nweighted avg       0.74      0.74      0.74    100000\n\nLightGBM Accuracy: 0.7389\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.05, iterations=50, depth=3, num_leaves=15...\nLightGBM Classification Report for learning_rate=0.05, iterations=50, depth=3, num_leaves=15:\n              precision    recall  f1-score   support\n\n           1       0.75      0.73      0.74     49906\n           2       0.74      0.76      0.75     50094\n\n    accuracy                           0.75    100000\n   macro avg       0.75      0.75      0.75    100000\nweighted avg       0.75      0.75      0.75    100000\n\nLightGBM Accuracy: 0.7465\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.05, iterations=50, depth=3, num_leaves=31...\nLightGBM Classification Report for learning_rate=0.05, iterations=50, depth=3, num_leaves=31:\n              precision    recall  f1-score   support\n\n           1       0.75      0.73      0.74     49906\n           2       0.74      0.76      0.75     50094\n\n    accuracy                           0.75    100000\n   macro avg       0.75      0.75      0.75    100000\nweighted avg       0.75      0.75      0.75    100000\n\nLightGBM Accuracy: 0.7465\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.05, iterations=50, depth=5, num_leaves=15...\nLightGBM Classification Report for learning_rate=0.05, iterations=50, depth=5, num_leaves=15:\n              precision    recall  f1-score   support\n\n           1       0.77      0.74      0.75     49906\n           2       0.75      0.78      0.77     50094\n\n    accuracy                           0.76    100000\n   macro avg       0.76      0.76      0.76    100000\nweighted avg       0.76      0.76      0.76    100000\n\nLightGBM Accuracy: 0.7603\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.05, iterations=50, depth=5, num_leaves=31...\nLightGBM Classification Report for learning_rate=0.05, iterations=50, depth=5, num_leaves=31:\n              precision    recall  f1-score   support\n\n           1       0.77      0.75      0.76     49906\n           2       0.76      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nLightGBM Accuracy: 0.7663\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.05, iterations=100, depth=3, num_leaves=15...\nLightGBM Classification Report for learning_rate=0.05, iterations=100, depth=3, num_leaves=15:\n              precision    recall  f1-score   support\n\n           1       0.78      0.75      0.77     49906\n           2       0.76      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nLightGBM Accuracy: 0.7693\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.05, iterations=100, depth=3, num_leaves=31...\nLightGBM Classification Report for learning_rate=0.05, iterations=100, depth=3, num_leaves=31:\n              precision    recall  f1-score   support\n\n           1       0.78      0.75      0.77     49906\n           2       0.76      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nLightGBM Accuracy: 0.7693\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.05, iterations=100, depth=5, num_leaves=15...\nLightGBM Classification Report for learning_rate=0.05, iterations=100, depth=5, num_leaves=15:\n              precision    recall  f1-score   support\n\n           1       0.79      0.76      0.78     49906\n           2       0.77      0.80      0.78     50094\n\n    accuracy                           0.78    100000\n   macro avg       0.78      0.78      0.78    100000\nweighted avg       0.78      0.78      0.78    100000\n\nLightGBM Accuracy: 0.7799\n\nMemory usage: 38.3%\nTraining LightGBM classifier with learning_rate=0.05, iterations=100, depth=5, num_leaves=31...\nLightGBM Classification Report for learning_rate=0.05, iterations=100, depth=5, num_leaves=31:\n              precision    recall  f1-score   support\n\n           1       0.79      0.77      0.78     49906\n           2       0.78      0.80      0.79     50094\n\n    accuracy                           0.78    100000\n   macro avg       0.78      0.78      0.78    100000\nweighted avg       0.78      0.78      0.78    100000\n\nLightGBM Accuracy: 0.7842\n\nMemory usage: 38.3%\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Adaboost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score\nimport psutil\nimport numpy as np\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Adjust target labels to be 0-based\ny_train_adjusted = y_train - 1\ny_test_adjusted = y_test - 1\n\n# Convert labels to numpy arrays\ny_train_adjusted = np.array(y_train_adjusted)\ny_test_adjusted = np.array(y_test_adjusted)\n\n# Define hyperparameter grid for AdaBoost\nn_estimators_values = [50, 100]\nlearning_rate_values = [0.01, 0.1]\n\n# Cross-validation setup (reduced folds to save memory)\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 3 folds\n\n# Iterate over hyperparameters\nfor n_estimators in n_estimators_values:\n    for learning_rate in learning_rate_values:\n        print(f\"Training AdaBoost classifier with n_estimators={n_estimators}, learning_rate={learning_rate}...\")\n\n        # Set up the AdaBoost model with the specified hyperparameters\n        adaboost_classifier = AdaBoostClassifier(\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=42\n        )\n\n        # Train the model\n        adaboost_classifier.fit(X_train_sbert[:10000], y_train_adjusted[:10000])  # Use a smaller subset\n        y_pred_adaboost = adaboost_classifier.predict(X_test_sbert)\n\n        # Adjust predictions back to the original label scale for reporting\n        y_pred_adaboost_original = y_pred_adaboost + 1\n\n        # Performance reporting\n        print(f\"AdaBoost Classification Report for n_estimators={n_estimators}, learning_rate={learning_rate}:\")\n        print(classification_report(y_test, y_pred_adaboost_original))\n        accuracy = accuracy_score(y_test, y_pred_adaboost_original)\n        print(f\"AdaBoost Accuracy: {accuracy:.4f}\\n\")\n\n        # Check memory usage after each iteration\n        check_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:01:56.935732Z","iopub.execute_input":"2025-02-01T16:01:56.936016Z","iopub.status.idle":"2025-02-01T16:04:40.344827Z","shell.execute_reply.started":"2025-02-01T16:01:56.935994Z","shell.execute_reply":"2025-02-01T16:04:40.344039Z"}},"outputs":[{"name":"stdout","text":"Training AdaBoost classifier with n_estimators=50, learning_rate=0.01...\nAdaBoost Classification Report for n_estimators=50, learning_rate=0.01:\n              precision    recall  f1-score   support\n\n           1       0.63      0.66      0.65     49906\n           2       0.65      0.61      0.63     50094\n\n    accuracy                           0.64    100000\n   macro avg       0.64      0.64      0.64    100000\nweighted avg       0.64      0.64      0.64    100000\n\nAdaBoost Accuracy: 0.6375\n\nMemory usage: 42.5%\nTraining AdaBoost classifier with n_estimators=50, learning_rate=0.1...\nAdaBoost Classification Report for n_estimators=50, learning_rate=0.1:\n              precision    recall  f1-score   support\n\n           1       0.72      0.71      0.72     49906\n           2       0.72      0.73      0.72     50094\n\n    accuracy                           0.72    100000\n   macro avg       0.72      0.72      0.72    100000\nweighted avg       0.72      0.72      0.72    100000\n\nAdaBoost Accuracy: 0.7218\n\nMemory usage: 42.5%\nTraining AdaBoost classifier with n_estimators=100, learning_rate=0.01...\nAdaBoost Classification Report for n_estimators=100, learning_rate=0.01:\n              precision    recall  f1-score   support\n\n           1       0.65      0.66      0.66     49906\n           2       0.66      0.65      0.66     50094\n\n    accuracy                           0.66    100000\n   macro avg       0.66      0.66      0.66    100000\nweighted avg       0.66      0.66      0.66    100000\n\nAdaBoost Accuracy: 0.6565\n\nMemory usage: 42.5%\nTraining AdaBoost classifier with n_estimators=100, learning_rate=0.1...\nAdaBoost Classification Report for n_estimators=100, learning_rate=0.1:\n              precision    recall  f1-score   support\n\n           1       0.75      0.74      0.75     49906\n           2       0.74      0.76      0.75     50094\n\n    accuracy                           0.75    100000\n   macro avg       0.75      0.75      0.75    100000\nweighted avg       0.75      0.75      0.75    100000\n\nAdaBoost Accuracy: 0.7491\n\nMemory usage: 42.6%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Catboost","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier, cv, Pool\nfrom sklearn.metrics import classification_report, accuracy_score\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport psutil\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\n\n# Load SBERT model\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Ensure all reviews are strings before splitting and handle missing values\ncombined_data['cleaned_review'] = combined_data['cleaned_review'].fillna('').astype(str)\nX = combined_data['cleaned_review']\ny = combined_data['label']\n\n# Reduce dataset size to prevent memory issues\nsample_size = min(500000, len(X))  # Limit to 500k samples if dataset is larger\nX_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, random_state=42)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42)\n\n# Compute SBERT embeddings for train and test sets\nX_train_sbert = np.array(sbert_model.encode(X_train.tolist(), batch_size=32, convert_to_numpy=True, normalize_embeddings=True))\nX_test_sbert = np.array(sbert_model.encode(X_test.tolist(), batch_size=32, convert_to_numpy=True, normalize_embeddings=True))\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Adjust target labels to be 0-based\ny_train_adjusted = y_train - 1\ny_test_adjusted = y_test - 1\n\n# Convert labels to numpy arrays\ny_train_adjusted = np.array(y_train_adjusted)\ny_test_adjusted = np.array(y_test_adjusted)\n\n# Define hyperparameter grid for CatBoost (with smaller iterations)\nlearning_rates = [0.01, 0.05]\niterations_values = [50, 100]  # Reduced iterations\ndepth_values = [3, 5]\nl2_leaf_reg_values = [1, 3]\nborder_count_values = [32, 50]\n\n# Cross-validation setup (reduced folds to save memory)\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 3 folds\n\n# Iterate over hyperparameters\nfor lr in learning_rates:\n    for iterations in iterations_values:\n        for depth in depth_values:\n            for l2_leaf_reg in l2_leaf_reg_values:\n                for border_count in border_count_values:\n                    print(f\"Training CatBoost classifier with learning_rate={lr}, iterations={iterations}, depth={depth}, l2_leaf_reg={l2_leaf_reg}, border_count={border_count}...\")\n\n                    # Set up the CatBoost model with the specified hyperparameters\n                    catboost_classifier = CatBoostClassifier(\n                        learning_rate=lr,\n                        iterations=iterations,\n                        depth=depth,\n                        l2_leaf_reg=l2_leaf_reg,\n                        border_count=border_count,\n                        verbose=0,  # To suppress verbose output\n                        thread_count=-1  # Use all available CPU cores\n                    )\n\n                    # Prepare data as Pool objects for CatBoost\n                    train_pool = Pool(X_train_sbert[:10000], label=y_train_adjusted[:10000])  # Use a smaller subset\n                    test_pool = Pool(X_test_sbert, label=y_test_adjusted)\n\n                    # Cross-validation to monitor the model's generalization\n                    params = catboost_classifier.get_params()\n                    params['eval_metric'] = 'Accuracy'  # Specify metric for evaluation\n                    params['loss_function'] = 'Logloss'  # Specify the loss function\n\n                    cv_results = cv(\n                        pool=train_pool,  # Provide the training data in Pool format\n                        params=params,  # Pass model parameters\n                        num_boost_round=iterations, \n                        nfold=3,  # Reduced folds to save memory\n                        early_stopping_rounds=10,  # Early stopping if performance doesn't improve\n                        as_pandas=True,\n                        seed=42\n                    )\n\n                    # Best iteration (round) from cross-validation results\n                    best_iter = cv_results['test-Accuracy-mean'].idxmax()  # Best round based on accuracy\n                    print(f\"Best round from CV: {best_iter}\")\n\n                    # Train the model with the best number of rounds\n                    catboost_classifier.fit(X_train_sbert, y_train_adjusted, verbose=0)\n\n                    print(\"Predicting with CatBoost...\")\n                    y_pred_catboost = catboost_classifier.predict(X_test_sbert)\n\n                    # Adjust predictions back to the original label scale for reporting\n                    y_pred_catboost_original = y_pred_catboost + 1\n\n                    # Performance reporting\n                    print(f\"CatBoost Classification Report for learning_rate={lr}, iterations={iterations}, depth={depth}, l2_leaf_reg={l2_leaf_reg}, border_count={border_count}:\")\n                    print(classification_report(y_test, y_pred_catboost_original))\n                    accuracy = accuracy_score(y_test, y_pred_catboost_original)\n                    print(f\"CatBoost Accuracy with learning_rate={lr}, iterations={iterations}, depth={depth}, l2_leaf_reg={l2_leaf_reg}, border_count={border_count}: {accuracy:.4f}\\n\")\n\n                    # Check memory usage after each iteration\n                    check_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:13:41.759028Z","iopub.execute_input":"2025-02-01T16:13:41.759354Z","iopub.status.idle":"2025-02-01T16:31:15.676133Z","shell.execute_reply.started":"2025-02-01T16:13:41.759330Z","shell.execute_reply":"2025-02-01T16:31:15.675355Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/12500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79dfaa5713f44c0da7dec156a0b400ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15388c0012954a7ca18120cd66cbdf36"}},"metadata":{}},{"name":"stdout","text":"Training CatBoost classifier with learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=1, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.700659868\nbestIteration = 49\n\nTraining on fold [1/3]\n\nbestTest = 0.6898620276\nbestIteration = 49\n\nTraining on fold [2/3]\n\nbestTest = 0.68727491\nbestIteration = 48\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=1, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.69      0.69      0.69     49906\n           2       0.69      0.69      0.69     50094\n\n    accuracy                           0.69    100000\n   macro avg       0.69      0.69      0.69    100000\nweighted avg       0.69      0.69      0.69    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=1, border_count=32: 0.6906\n\nMemory usage: 57.2%\nTraining CatBoost classifier with learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=1, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.6910617876\nbestIteration = 16\n\nTraining on fold [1/3]\n\nbestTest = 0.6931613677\nbestIteration = 48\n\nTraining on fold [2/3]\n\nbestTest = 0.6761704682\nbestIteration = 13\n\nBest round from CV: 48\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=1, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.69      0.70      0.69     49906\n           2       0.69      0.68      0.69     50094\n\n    accuracy                           0.69    100000\n   macro avg       0.69      0.69      0.69    100000\nweighted avg       0.69      0.69      0.69    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=1, border_count=50: 0.6895\n\nMemory usage: 57.6%\nTraining CatBoost classifier with learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=3, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.6943611278\nbestIteration = 10\n\nTraining on fold [1/3]\n\nbestTest = 0.6961607678\nbestIteration = 49\n\nTraining on fold [2/3]\n\nbestTest = 0.6671668667\nbestIteration = 12\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=3, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.69      0.70      0.69     49906\n           2       0.69      0.69      0.69     50094\n\n    accuracy                           0.69    100000\n   macro avg       0.69      0.69      0.69    100000\nweighted avg       0.69      0.69      0.69    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=3, border_count=32: 0.6921\n\nMemory usage: 57.8%\nTraining CatBoost classifier with learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=3, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.699460108\nbestIteration = 32\n\nTraining on fold [1/3]\n\nbestTest = 0.6856628674\nbestIteration = 48\n\nTraining on fold [2/3]\n\nbestTest = 0.6884753902\nbestIteration = 49\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=3, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.69      0.69      0.69     49906\n           2       0.69      0.68      0.69     50094\n\n    accuracy                           0.69    100000\n   macro avg       0.69      0.69      0.69    100000\nweighted avg       0.69      0.69      0.69    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=50, depth=3, l2_leaf_reg=3, border_count=50: 0.6894\n\nMemory usage: 57.8%\nTraining CatBoost classifier with learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=1, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.726154769\nbestIteration = 48\n\nTraining on fold [1/3]\n\nbestTest = 0.7135572885\nbestIteration = 49\n\nTraining on fold [2/3]\n\nbestTest = 0.7145858343\nbestIteration = 48\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=1, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.72      0.71      0.72     49906\n           2       0.72      0.72      0.72     50094\n\n    accuracy                           0.72    100000\n   macro avg       0.72      0.72      0.72    100000\nweighted avg       0.72      0.72      0.72    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=1, border_count=32: 0.7165\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=1, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7264547091\nbestIteration = 28\n\nTraining on fold [1/3]\n\nbestTest = 0.7156568686\nbestIteration = 48\n\nTraining on fold [2/3]\n\nbestTest = 0.7169867947\nbestIteration = 48\n\nBest round from CV: 48\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=1, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.72      0.72      0.72     49906\n           2       0.72      0.72      0.72     50094\n\n    accuracy                           0.72    100000\n   macro avg       0.72      0.72      0.72    100000\nweighted avg       0.72      0.72      0.72    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=1, border_count=50: 0.7191\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=3, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.725854829\nbestIteration = 8\n\nTraining on fold [1/3]\n\nbestTest = 0.7156568686\nbestIteration = 49\n\nTraining on fold [2/3]\n\nbestTest = 0.718487395\nbestIteration = 45\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=3, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.72      0.71      0.72     49906\n           2       0.72      0.72      0.72     50094\n\n    accuracy                           0.72    100000\n   macro avg       0.72      0.72      0.72    100000\nweighted avg       0.72      0.72      0.72    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=3, border_count=32: 0.7176\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=3, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7204559088\nbestIteration = 23\n\nTraining on fold [1/3]\n\nbestTest = 0.7153569286\nbestIteration = 48\n\nTraining on fold [2/3]\n\nbestTest = 0.7106842737\nbestIteration = 48\n\nBest round from CV: 48\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=3, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.72      0.71      0.72     49906\n           2       0.72      0.72      0.72     50094\n\n    accuracy                           0.72    100000\n   macro avg       0.72      0.72      0.72    100000\nweighted avg       0.72      0.72      0.72    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=50, depth=5, l2_leaf_reg=3, border_count=50: 0.7184\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=1, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7222555489\nbestIteration = 98\n\nTraining on fold [1/3]\n\nbestTest = 0.7111577684\nbestIteration = 99\n\nTraining on fold [2/3]\n\nbestTest = 0.7010804322\nbestIteration = 76\n\nBest round from CV: 99\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=1, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.71      0.71      0.71     49906\n           2       0.71      0.71      0.71     50094\n\n    accuracy                           0.71    100000\n   macro avg       0.71      0.71      0.71    100000\nweighted avg       0.71      0.71      0.71    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=1, border_count=32: 0.7107\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=1, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.6910617876\nbestIteration = 16\n\nTraining on fold [1/3]\n\nbestTest = 0.6931613677\nbestIteration = 48\n\nTraining on fold [2/3]\n\nbestTest = 0.6761704682\nbestIteration = 13\n\nBest round from CV: 48\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=1, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.71      0.71      0.71     49906\n           2       0.71      0.71      0.71     50094\n\n    accuracy                           0.71    100000\n   macro avg       0.71      0.71      0.71    100000\nweighted avg       0.71      0.71      0.71    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=1, border_count=50: 0.7104\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=3, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.6943611278\nbestIteration = 10\n\nTraining on fold [1/3]\n\nbestTest = 0.7117576485\nbestIteration = 91\n\nTraining on fold [2/3]\n\nbestTest = 0.6671668667\nbestIteration = 12\n\nBest round from CV: 91\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=3, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.71      0.71      0.71     49906\n           2       0.71      0.71      0.71     50094\n\n    accuracy                           0.71    100000\n   macro avg       0.71      0.71      0.71    100000\nweighted avg       0.71      0.71      0.71    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=3, border_count=32: 0.7103\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=3, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.699460108\nbestIteration = 32\n\nTraining on fold [1/3]\n\nbestTest = 0.7108578284\nbestIteration = 91\n\nTraining on fold [2/3]\n\nbestTest = 0.7130852341\nbestIteration = 97\n\nBest round from CV: 97\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=3, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.71      0.71      0.71     49906\n           2       0.71      0.71      0.71     50094\n\n    accuracy                           0.71    100000\n   macro avg       0.71      0.71      0.71    100000\nweighted avg       0.71      0.71      0.71    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=100, depth=3, l2_leaf_reg=3, border_count=50: 0.7103\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=1, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7435512897\nbestIteration = 98\n\nTraining on fold [1/3]\n\nbestTest = 0.7294541092\nbestIteration = 70\n\nTraining on fold [2/3]\n\nbestTest = 0.7385954382\nbestIteration = 99\n\nBest round from CV: 99\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=1, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.74      0.73      0.74     49906\n           2       0.74      0.74      0.74     50094\n\n    accuracy                           0.74    100000\n   macro avg       0.74      0.74      0.74    100000\nweighted avg       0.74      0.74      0.74    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=1, border_count=32: 0.7372\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=1, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7264547091\nbestIteration = 28\n\nTraining on fold [1/3]\n\nbestTest = 0.7294541092\nbestIteration = 69\n\nTraining on fold [2/3]\n\nbestTest = 0.7208883553\nbestIteration = 54\n\nBest round from CV: 69\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=1, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.74      0.73      0.73     49906\n           2       0.73      0.74      0.74     50094\n\n    accuracy                           0.74    100000\n   macro avg       0.74      0.74      0.74    100000\nweighted avg       0.74      0.74      0.74    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=1, border_count=50: 0.7365\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=3, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.725854829\nbestIteration = 8\n\nTraining on fold [1/3]\n\nbestTest = 0.7315536893\nbestIteration = 72\n\nTraining on fold [2/3]\n\nbestTest = 0.7322929172\nbestIteration = 99\n\nBest round from CV: 99\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=3, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.74      0.73      0.73     49906\n           2       0.73      0.74      0.74     50094\n\n    accuracy                           0.74    100000\n   macro avg       0.74      0.74      0.74    100000\nweighted avg       0.74      0.74      0.74    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=3, border_count=32: 0.7371\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=3, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7204559088\nbestIteration = 23\n\nTraining on fold [1/3]\n\nbestTest = 0.7222555489\nbestIteration = 68\n\nTraining on fold [2/3]\n\nbestTest = 0.7370948379\nbestIteration = 99\n\nBest round from CV: 99\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=3, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.74      0.73      0.73     49906\n           2       0.73      0.74      0.74     50094\n\n    accuracy                           0.74    100000\n   macro avg       0.74      0.74      0.74    100000\nweighted avg       0.74      0.74      0.74    100000\n\nCatBoost Accuracy with learning_rate=0.01, iterations=100, depth=5, l2_leaf_reg=3, border_count=50: 0.7361\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=1, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.75074985\nbestIteration = 49\n\nTraining on fold [1/3]\n\nbestTest = 0.7426514697\nbestIteration = 46\n\nTraining on fold [2/3]\n\nbestTest = 0.743697479\nbestIteration = 49\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=1, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.75      0.74      0.74     49906\n           2       0.74      0.75      0.75     50094\n\n    accuracy                           0.75    100000\n   macro avg       0.75      0.75      0.75    100000\nweighted avg       0.75      0.75      0.75    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=1, border_count=32: 0.7461\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=1, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7522495501\nbestIteration = 47\n\nTraining on fold [1/3]\n\nbestTest = 0.7474505099\nbestIteration = 49\n\nTraining on fold [2/3]\n\nbestTest = 0.7493997599\nbestIteration = 49\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=1, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.75      0.74      0.74     49906\n           2       0.74      0.75      0.75     50094\n\n    accuracy                           0.75    100000\n   macro avg       0.75      0.75      0.75    100000\nweighted avg       0.75      0.75      0.75    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=1, border_count=50: 0.7457\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=3, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7471505699\nbestIteration = 48\n\nTraining on fold [1/3]\n\nbestTest = 0.7465506899\nbestIteration = 48\n\nTraining on fold [2/3]\n\nbestTest = 0.7460984394\nbestIteration = 48\n\nBest round from CV: 48\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=3, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.75      0.74      0.74     49906\n           2       0.74      0.75      0.75     50094\n\n    accuracy                           0.75    100000\n   macro avg       0.75      0.75      0.75    100000\nweighted avg       0.75      0.75      0.75    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=3, border_count=32: 0.7465\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=3, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7531493701\nbestIteration = 49\n\nTraining on fold [1/3]\n\nbestTest = 0.7453509298\nbestIteration = 46\n\nTraining on fold [2/3]\n\nbestTest = 0.7382953181\nbestIteration = 47\n\nBest round from CV: 47\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=3, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.75      0.74      0.74     49906\n           2       0.74      0.75      0.75     50094\n\n    accuracy                           0.75    100000\n   macro avg       0.75      0.75      0.75    100000\nweighted avg       0.75      0.75      0.75    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=50, depth=3, l2_leaf_reg=3, border_count=50: 0.7459\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=1, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7723455309\nbestIteration = 49\n\nTraining on fold [1/3]\n\nbestTest = 0.7648470306\nbestIteration = 40\n\nTraining on fold [2/3]\n\nbestTest = 0.7581032413\nbestIteration = 45\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=1, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.77      0.76      0.76     49906\n           2       0.76      0.77      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=1, border_count=32: 0.7653\n\nMemory usage: 57.9%\nTraining CatBoost classifier with learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=1, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7648470306\nbestIteration = 32\n\nTraining on fold [1/3]\n\nbestTest = 0.7690461908\nbestIteration = 49\n\nTraining on fold [2/3]\n\nbestTest = 0.7620048019\nbestIteration = 48\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=1, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.77      0.76      0.76     49906\n           2       0.76      0.77      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=1, border_count=50: 0.7661\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=3, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7735452909\nbestIteration = 47\n\nTraining on fold [1/3]\n\nbestTest = 0.7582483503\nbestIteration = 49\n\nTraining on fold [2/3]\n\nbestTest = 0.7674069628\nbestIteration = 49\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=3, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.77      0.76      0.76     49906\n           2       0.76      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=3, border_count=32: 0.7660\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=3, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7735452909\nbestIteration = 48\n\nTraining on fold [1/3]\n\nbestTest = 0.7663467307\nbestIteration = 47\n\nTraining on fold [2/3]\n\nbestTest = 0.762304922\nbestIteration = 49\n\nBest round from CV: 49\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=3, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.77      0.76      0.76     49906\n           2       0.76      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=50, depth=5, l2_leaf_reg=3, border_count=50: 0.7662\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=1, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7657468506\nbestIteration = 71\n\nTraining on fold [1/3]\n\nbestTest = 0.7675464907\nbestIteration = 80\n\nTraining on fold [2/3]\n\nbestTest = 0.7599039616\nbestIteration = 56\n\nBest round from CV: 80\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=1, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.77      0.76      0.77     49906\n           2       0.77      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=1, border_count=32: 0.7701\n\nMemory usage: 58.1%\nTraining CatBoost classifier with learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=1, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.775644871\nbestIteration = 93\n\nTraining on fold [1/3]\n\nbestTest = 0.7702459508\nbestIteration = 98\n\nTraining on fold [2/3]\n\nbestTest = 0.7719087635\nbestIteration = 99\n\nBest round from CV: 98\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=1, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.77      0.76      0.77     49906\n           2       0.77      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=1, border_count=50: 0.7704\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=3, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7711457708\nbestIteration = 86\n\nTraining on fold [1/3]\n\nbestTest = 0.7672465507\nbestIteration = 94\n\nTraining on fold [2/3]\n\nbestTest = 0.7716086435\nbestIteration = 92\n\nBest round from CV: 94\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=3, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.77      0.76      0.77     49906\n           2       0.77      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=3, border_count=32: 0.7703\n\nMemory usage: 58.1%\nTraining CatBoost classifier with learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=3, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.775044991\nbestIteration = 95\n\nTraining on fold [1/3]\n\nbestTest = 0.7675464907\nbestIteration = 99\n\nTraining on fold [2/3]\n\nbestTest = 0.768907563\nbestIteration = 95\n\nBest round from CV: 99\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=3, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.77      0.76      0.77     49906\n           2       0.77      0.78      0.77     50094\n\n    accuracy                           0.77    100000\n   macro avg       0.77      0.77      0.77    100000\nweighted avg       0.77      0.77      0.77    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=100, depth=3, l2_leaf_reg=3, border_count=50: 0.7705\n\nMemory usage: 58.1%\nTraining CatBoost classifier with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7858428314\nbestIteration = 98\n\nTraining on fold [1/3]\n\nbestTest = 0.776244751\nbestIteration = 70\n\nTraining on fold [2/3]\n\nbestTest = 0.7869147659\nbestIteration = 98\n\nBest round from CV: 98\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.79      0.78      0.78     49906\n           2       0.78      0.79      0.79     50094\n\n    accuracy                           0.78    100000\n   macro avg       0.78      0.78      0.78    100000\nweighted avg       0.78      0.78      0.78    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=32: 0.7847\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7648470306\nbestIteration = 32\n\nTraining on fold [1/3]\n\nbestTest = 0.774445111\nbestIteration = 59\n\nTraining on fold [2/3]\n\nbestTest = 0.787214886\nbestIteration = 91\n\nBest round from CV: 91\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.79      0.78      0.78     49906\n           2       0.78      0.79      0.79     50094\n\n    accuracy                           0.78    100000\n   macro avg       0.78      0.78      0.78    100000\nweighted avg       0.78      0.78      0.78    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=1, border_count=50: 0.7845\n\nMemory usage: 58.1%\nTraining CatBoost classifier with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=32...\nTraining on fold [0/3]\n\nbestTest = 0.7828434313\nbestIteration = 98\n\nTraining on fold [1/3]\n\nbestTest = 0.7870425915\nbestIteration = 97\n\nTraining on fold [2/3]\n\nbestTest = 0.7776110444\nbestIteration = 80\n\nBest round from CV: 97\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=32:\n              precision    recall  f1-score   support\n\n           1       0.79      0.78      0.78     49906\n           2       0.78      0.79      0.79     50094\n\n    accuracy                           0.78    100000\n   macro avg       0.78      0.78      0.78    100000\nweighted avg       0.78      0.78      0.78    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=32: 0.7840\n\nMemory usage: 58.0%\nTraining CatBoost classifier with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=50...\nTraining on fold [0/3]\n\nbestTest = 0.7915416917\nbestIteration = 88\n\nTraining on fold [1/3]\n\nbestTest = 0.7864427115\nbestIteration = 81\n\nTraining on fold [2/3]\n\nbestTest = 0.7830132053\nbestIteration = 97\n\nBest round from CV: 98\nPredicting with CatBoost...\nCatBoost Classification Report for learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=50:\n              precision    recall  f1-score   support\n\n           1       0.79      0.78      0.78     49906\n           2       0.78      0.79      0.79     50094\n\n    accuracy                           0.78    100000\n   macro avg       0.78      0.78      0.78    100000\nweighted avg       0.78      0.78      0.78    100000\n\nCatBoost Accuracy with learning_rate=0.05, iterations=100, depth=5, l2_leaf_reg=3, border_count=50: 0.7847\n\nMemory usage: 58.0%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\nimport psutil\n\n# Load SBERT model\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Ensure all reviews are strings before splitting and handle missing values\ncombined_data['cleaned_review'] = combined_data['cleaned_review'].fillna('').astype(str)\nX = combined_data['cleaned_review']\ny = combined_data['label']\n\n# Reduce dataset size to prevent memory issues\nsample_size = min(500000, len(X))  # Limit to 500k samples if dataset is larger\nX_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, random_state=42)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42)\n\n# Compute SBERT embeddings for train and test sets\nX_train_sbert = np.array(sbert_model.encode(X_train.tolist(), batch_size=32, convert_to_numpy=True, normalize_embeddings=True))\nX_test_sbert = np.array(sbert_model.encode(X_test.tolist(), batch_size=32, convert_to_numpy=True, normalize_embeddings=True))\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Adjust target labels to be 0-based\ny_train_adjusted = y_train - 1\ny_test_adjusted = y_test - 1\n\n# Convert labels to numpy arrays\ny_train_adjusted = np.array(y_train_adjusted)\ny_test_adjusted = np.array(y_test_adjusted)\n\n# Define hyperparameter grid for SVM\nC_values = [0.1, 1, 10]\nkernel_values = ['linear', 'rbf']\ngamma_values = ['scale', 'auto']\n\n# Cross-validation setup (reduced folds to save memory)\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 3 folds\n\n# Iterate over hyperparameters\nfor C in C_values:\n    for kernel in kernel_values:\n        for gamma in gamma_values:\n            print(f\"Training SVM classifier with C={C}, kernel={kernel}, gamma={gamma}...\")\n\n            # Set up the SVM model with the specified hyperparameters\n            svm_classifier = SVC(\n                C=C,\n                kernel=kernel,\n                gamma=gamma,\n                random_state=42\n            )\n\n            # Train the model\n            svm_classifier.fit(X_train_sbert[:10000], y_train_adjusted[:10000])  # Use a smaller subset\n            y_pred_svm = svm_classifier.predict(X_test_sbert)\n\n            # Adjust predictions back to the original label scale for reporting\n            y_pred_svm_original = y_pred_svm + 1\n\n            # Performance reporting\n            print(f\"SVM Classification Report for C={C}, kernel={kernel}, gamma={gamma}:\")\n            print(classification_report(y_test, y_pred_svm_original))\n            accuracy = accuracy_score(y_test, y_pred_svm_original)\n            print(f\"SVM Accuracy with C={C}, kernel={kernel}, gamma={gamma}: {accuracy:.4f}\\n\")\n\n            # Check memory usage after each iteration\n            check_memory()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Naive bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\nimport psutil\n\n# Load SBERT model\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Ensure all reviews are strings before splitting and handle missing values\ncombined_data['cleaned_review'] = combined_data['cleaned_review'].fillna('').astype(str)\nX = combined_data['cleaned_review']\ny = combined_data['label']\n\n# Reduce dataset size to prevent memory issues\nsample_size = min(500000, len(X))  # Limit to 500k samples if dataset is larger\nX_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, random_state=42)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42)\n\n# Compute SBERT embeddings for train and test sets\nX_train_sbert = np.array(sbert_model.encode(X_train.tolist(), batch_size=32, convert_to_numpy=True, normalize_embeddings=True))\nX_test_sbert = np.array(sbert_model.encode(X_test.tolist(), batch_size=32, convert_to_numpy=True, normalize_embeddings=True))\n\n# Apply ReLU activation to make all values non-negative\nX_train_sbert = np.maximum(0, X_train_sbert)\nX_test_sbert = np.maximum(0, X_test_sbert)\n\n# Function to check memory usage\ndef check_memory():\n    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Adjust target labels to be 0-based\ny_train_adjusted = y_train - 1\ny_test_adjusted = y_test - 1\n\n# Convert labels to numpy arrays\ny_train_adjusted = np.array(y_train_adjusted)\ny_test_adjusted = np.array(y_test_adjusted)\n\n# Define hyperparameter grid for Naive Bayes\nalpha_values = [0.01, 0.1, 1.0]  # Regularization parameter for Naive Bayes\n\n# Cross-validation setup (reduced folds to save memory)\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 3 folds\n\n# Iterate over hyperparameters\nfor alpha in alpha_values:\n    print(f\"Training Naive Bayes classifier with alpha={alpha}...\")\n\n    # Set up the Naive Bayes model with the specified hyperparameters\n    nb_classifier = MultinomialNB(alpha=alpha)\n\n    # Train the model\n    nb_classifier.fit(X_train_sbert[:10000], y_train_adjusted[:10000])  # Use a smaller subset\n    y_pred_nb = nb_classifier.predict(X_test_sbert)\n\n    # Adjust predictions back to the original label scale for reporting\n    y_pred_nb_original = y_pred_nb + 1\n\n    # Performance reporting\n    print(f\"Naive Bayes Classification Report for alpha={alpha}:\")\n    print(classification_report(y_test, y_pred_nb_original))\n    accuracy = accuracy_score(y_test, y_pred_nb_original)\n    print(f\"Naive Bayes Accuracy with alpha={alpha}: {accuracy:.4f}\\n\")\n\n    # Check memory usage after each iteration\n    check_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:25:27.773166Z","iopub.execute_input":"2025-02-01T17:25:27.773563Z","iopub.status.idle":"2025-02-01T17:30:07.241967Z","shell.execute_reply.started":"2025-02-01T17:25:27.773533Z","shell.execute_reply":"2025-02-01T17:30:07.241029Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/12500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca58492b0f745c0be7e1e6d05102d6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9ba33e8cca34e689dd2c38973232004"}},"metadata":{}},{"name":"stdout","text":"Training Naive Bayes classifier with alpha=0.01...\nNaive Bayes Classification Report for alpha=0.01:\n              precision    recall  f1-score   support\n\n           1       0.78      0.71      0.75     49906\n           2       0.74      0.80      0.77     50094\n\n    accuracy                           0.76    100000\n   macro avg       0.76      0.76      0.76    100000\nweighted avg       0.76      0.76      0.76    100000\n\nNaive Bayes Accuracy with alpha=0.01: 0.7589\n\nMemory usage: 74.4%\nTraining Naive Bayes classifier with alpha=0.1...\nNaive Bayes Classification Report for alpha=0.1:\n              precision    recall  f1-score   support\n\n           1       0.78      0.71      0.75     49906\n           2       0.74      0.80      0.77     50094\n\n    accuracy                           0.76    100000\n   macro avg       0.76      0.76      0.76    100000\nweighted avg       0.76      0.76      0.76    100000\n\nNaive Bayes Accuracy with alpha=0.1: 0.7589\n\nMemory usage: 74.4%\nTraining Naive Bayes classifier with alpha=1.0...\nNaive Bayes Classification Report for alpha=1.0:\n              precision    recall  f1-score   support\n\n           1       0.78      0.71      0.75     49906\n           2       0.74      0.80      0.77     50094\n\n    accuracy                           0.76    100000\n   macro avg       0.76      0.76      0.76    100000\nweighted avg       0.76      0.76      0.76    100000\n\nNaive Bayes Accuracy with alpha=1.0: 0.7589\n\nMemory usage: 74.4%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}